{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "name": "Neptune_cross_validation.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python (neptune)",
      "language": "python",
      "name": "neptune"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "THIzFFk81nme"
      },
      "source": [
        "# How to present CV with Neptune\n",
        "\n",
        "## Before you start\n",
        "\n",
        "### Install dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "54CFqKRANLK5",
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a19f2835-22ab-41c9-ab13-e6e4d8fdbea9"
      },
      "source": [
        "! pip install neptune-client numpy==1.19.5 torch torchvision sklearn"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting neptune-client\n",
            "  Downloading neptune-client-0.13.0.tar.gz (278 kB)\n",
            "\u001b[?25l\r\u001b[K     |█▏                              | 10 kB 19.7 MB/s eta 0:00:01\r\u001b[K     |██▍                             | 20 kB 21.7 MB/s eta 0:00:01\r\u001b[K     |███▌                            | 30 kB 15.1 MB/s eta 0:00:01\r\u001b[K     |████▊                           | 40 kB 10.2 MB/s eta 0:00:01\r\u001b[K     |██████                          | 51 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████                         | 61 kB 6.0 MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 71 kB 5.8 MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 81 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 92 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 102 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 112 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 122 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 133 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 143 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 153 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 163 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 174 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 184 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 194 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 204 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 215 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 225 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 235 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 245 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 256 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 266 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 276 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 278 kB 5.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy==1.19.5 in /usr/local/lib/python3.7/dist-packages (1.19.5)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.9.0+cu111)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (0.10.0+cu111)\n",
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.7/dist-packages (0.0)\n",
            "Collecting bravado\n",
            "  Downloading bravado-11.0.3-py2.py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.7/dist-packages (from neptune-client) (7.1.2)\n",
            "Collecting future>=0.17.1\n",
            "  Downloading future-0.18.2.tar.gz (829 kB)\n",
            "\u001b[K     |████████████████████████████████| 829 kB 44.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: oauthlib>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from neptune-client) (3.1.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from neptune-client) (1.1.5)\n",
            "Requirement already satisfied: Pillow>=1.1.6 in /usr/local/lib/python3.7/dist-packages (from neptune-client) (7.1.2)\n",
            "Collecting PyJWT\n",
            "  Downloading PyJWT-2.3.0-py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: requests>=2.20.0 in /usr/local/lib/python3.7/dist-packages (from neptune-client) (2.23.0)\n",
            "Requirement already satisfied: requests-oauthlib>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from neptune-client) (1.3.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from neptune-client) (1.15.0)\n",
            "Collecting websocket-client!=1.0.0,>=0.35.0\n",
            "  Downloading websocket_client-1.2.1-py2.py3-none-any.whl (52 kB)\n",
            "\u001b[K     |████████████████████████████████| 52 kB 995 kB/s \n",
            "\u001b[?25hCollecting GitPython>=2.0.8\n",
            "  Downloading GitPython-3.1.24-py3-none-any.whl (180 kB)\n",
            "\u001b[K     |████████████████████████████████| 180 kB 51.9 MB/s \n",
            "\u001b[?25hCollecting boto3>=1.16.0\n",
            "  Downloading boto3-1.19.4-py3-none-any.whl (131 kB)\n",
            "\u001b[K     |████████████████████████████████| 131 kB 49.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from neptune-client) (21.0)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from neptune-client) (1.24.3)\n",
            "Requirement already satisfied: jsonschema<4 in /usr/local/lib/python3.7/dist-packages (from neptune-client) (2.6.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from neptune-client) (5.4.8)\n",
            "Collecting s3transfer<0.6.0,>=0.5.0\n",
            "  Downloading s3transfer-0.5.0-py3-none-any.whl (79 kB)\n",
            "\u001b[K     |████████████████████████████████| 79 kB 5.9 MB/s \n",
            "\u001b[?25hCollecting botocore<1.23.0,>=1.22.4\n",
            "  Downloading botocore-1.22.4-py3-none-any.whl (8.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 8.0 MB 39.5 MB/s \n",
            "\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1\n",
            "  Downloading jmespath-0.10.0-py2.py3-none-any.whl (24 kB)\n",
            "Collecting urllib3\n",
            "  Downloading urllib3-1.26.7-py2.py3-none-any.whl (138 kB)\n",
            "\u001b[K     |████████████████████████████████| 138 kB 58.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.23.0,>=1.22.4->boto3>=1.16.0->neptune-client) (2.8.2)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 1.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from GitPython>=2.0.8->neptune-client) (3.7.4.3)\n",
            "Collecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20.0->neptune-client) (2.10)\n",
            "Collecting urllib3\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[K     |████████████████████████████████| 127 kB 45.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20.0->neptune-client) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20.0->neptune-client) (3.0.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sklearn) (0.22.2.post1)\n",
            "Collecting bravado-core>=5.16.1\n",
            "  Downloading bravado_core-5.17.0-py2.py3-none-any.whl (67 kB)\n",
            "\u001b[K     |████████████████████████████████| 67 kB 4.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: msgpack in /usr/local/lib/python3.7/dist-packages (from bravado->neptune-client) (1.0.2)\n",
            "Collecting simplejson\n",
            "  Downloading simplejson-3.17.5-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (129 kB)\n",
            "\u001b[K     |████████████████████████████████| 129 kB 63.5 MB/s \n",
            "\u001b[?25hCollecting monotonic\n",
            "  Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from bravado->neptune-client) (3.13)\n",
            "Collecting swagger-spec-validator>=2.0.1\n",
            "  Downloading swagger_spec_validator-2.7.3-py2.py3-none-any.whl (27 kB)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from bravado-core>=5.16.1->bravado->neptune-client) (2018.9)\n",
            "Collecting jsonref\n",
            "  Downloading jsonref-0.2-py3-none-any.whl (9.3 kB)\n",
            "Collecting rfc3987\n",
            "  Downloading rfc3987-1.3.8-py2.py3-none-any.whl (13 kB)\n",
            "Collecting webcolors\n",
            "  Downloading webcolors-1.11.1-py3-none-any.whl (9.9 kB)\n",
            "Collecting strict-rfc3339\n",
            "  Downloading strict-rfc3339-0.7.tar.gz (17 kB)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->neptune-client) (2.4.7)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.0.1)\n",
            "Building wheels for collected packages: neptune-client, future, strict-rfc3339\n",
            "  Building wheel for neptune-client (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for neptune-client: filename=neptune_client-0.13.0-py2.py3-none-any.whl size=482068 sha256=9ac3dc92d0f56695d1048861a084307a5a8b7c1629d46e8b6bd6d6879eb814ee\n",
            "  Stored in directory: /root/.cache/pip/wheels/66/62/8a/356e365defb466fb97dd3bf11b45004138549cef8bdd2cf7b7\n",
            "  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for future: filename=future-0.18.2-py3-none-any.whl size=491070 sha256=0d5c990611f1937c91165f776640e05da5d4d4ac7d7caeba0e89582626270029\n",
            "  Stored in directory: /root/.cache/pip/wheels/56/b0/fe/4410d17b32f1f0c3cf54cdfb2bc04d7b4b8f4ae377e2229ba0\n",
            "  Building wheel for strict-rfc3339 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for strict-rfc3339: filename=strict_rfc3339-0.7-py3-none-any.whl size=18149 sha256=31689be63f2b00e2f869ba8c6cc8bce11d904f38e28fd0f0df33cbd90a07816c\n",
            "  Stored in directory: /root/.cache/pip/wheels/f3/1d/9f/2a74caecb81b8beb9a4fbe1754203d4b7cf42ef5d39e0d2311\n",
            "Successfully built neptune-client future strict-rfc3339\n",
            "Installing collected packages: webcolors, urllib3, strict-rfc3339, rfc3987, jmespath, swagger-spec-validator, smmap, simplejson, jsonref, botocore, s3transfer, monotonic, gitdb, bravado-core, websocket-client, PyJWT, GitPython, future, bravado, boto3, neptune-client\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "  Attempting uninstall: future\n",
            "    Found existing installation: future 0.16.0\n",
            "    Uninstalling future-0.16.0:\n",
            "      Successfully uninstalled future-0.16.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed GitPython-3.1.24 PyJWT-2.3.0 boto3-1.19.4 botocore-1.22.4 bravado-11.0.3 bravado-core-5.17.0 future-0.18.2 gitdb-4.0.9 jmespath-0.10.0 jsonref-0.2 monotonic-1.6 neptune-client-0.13.0 rfc3987-1.3.8 s3transfer-0.5.0 simplejson-3.17.5 smmap-5.0.0 strict-rfc3339-0.7 swagger-spec-validator-2.7.3 urllib3-1.25.11 webcolors-1.11.1 websocket-client-1.2.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "32j_CnJiE7V0",
        "outputId": "02d5f1bf-91f7-4586-e4a0-1ae3c006d51b"
      },
      "source": [
        "import neptune.new as neptune\n",
        "run = neptune.init(\n",
        "    project = 'common/showroom', \n",
        "    api_token = 'ANONYMOUS'\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Info (NVML): Driver Not Loaded. GPU usage metrics may not be reported. For more information, see https://docs-legacy.neptune.ai/logging-and-managing-experiment-results/logging-experiment-data.html#hardware-consumption \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://app.neptune.ai/common/showroom/e/SHOW-3414\n",
            "Remember to stop your run once you’ve finished logging your metadata (https://docs.neptune.ai/api-reference/run#stop). It will be stopped automatically only when the notebook kernel/interactive console is terminated.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HVuAP8dxFhQC"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XG6-BmFnXR7J",
        "outputId": "0dbcd7ff-16ed-414f-eedc-ab26193d338c"
      },
      "source": [
        "import neptune.new as neptune\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import ConcatDataset, SubsetRandomSampler, DataLoader\n",
        "from sklearn.model_selection import KFold\n",
        "from statistics import mean\n",
        "# Create Run\n",
        "run = neptune.init(\n",
        "    project = 'common/showroom', \n",
        "    api_token = 'ANONYMOUS'\n",
        ")\n",
        "parameters = {\n",
        "    \"epochs\": 1,\n",
        "    \"lr\": 1e-2,\n",
        "    \"bs\": 10,\n",
        "    \"input_sz\": 32 * 32 * 3,\n",
        "    \"n_classes\": 10,\n",
        "    \"k_folds\": 5,\n",
        "    \"model_name\": \"checkpoint.pth\",\n",
        "    \"device\": torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"),\n",
        "    \"seed\": 42,\n",
        "}\n",
        "\n",
        "# Log hyperparameters\n",
        "run[\"global/parameters\"] = parameters\n",
        "\n",
        "# Seed\n",
        "torch.manual_seed(parameters['seed'])\n",
        "\n",
        "# Model\n",
        "class BaseModel(nn.Module):\n",
        "    def __init__(self, input_sz, hidden_dim, n_classes):\n",
        "        super(BaseModel, self).__init__()\n",
        "        self.main = nn.Sequential(\n",
        "            nn.Linear(input_sz, hidden_dim * 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim // 2, n_classes),\n",
        "        )\n",
        "    def forward(self, input):\n",
        "        x = input.view(-1, 32 * 32 * 3)\n",
        "        return self.main(x)\n",
        "        \n",
        "model = BaseModel(\n",
        "    parameters[\"input_sz\"], parameters[\"input_sz\"], parameters[\"n_classes\"]\n",
        ").to(parameters[\"device\"])\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=parameters[\"lr\"])\n",
        "\n",
        "# trainset\n",
        "data_dir = \"data/CIFAR10\"\n",
        "compressed_ds = \"./data/CIFAR10/cifar-10-python.tar.gz\"\n",
        "data_tfms = {\n",
        "    \"train\": transforms.Compose(\n",
        "        [\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
        "        ]\n",
        "    )\n",
        "}\n",
        "trainset = datasets.CIFAR10(data_dir, transform=data_tfms[\"train\"], download=True)\n",
        "\n",
        "splits = KFold(n_splits=parameters['k_folds'], shuffle=True)\n",
        "epoch_acc_list, epoch_loss_list= [], []\n",
        "\n",
        "for fold, (train_ids, _ ) in enumerate(splits.split(trainset)):\n",
        "    train_sampler = SubsetRandomSampler(train_ids)\n",
        "    train_loader = DataLoader(trainset, batch_size=parameters['bs'], sampler=train_sampler)\n",
        "    for epoch in range(parameters[\"epochs\"]): \n",
        "        epoch_acc, epoch_loss= 0, 0.0\n",
        "        for x, y in train_loader:\n",
        "            x, y = x.to(parameters[\"device\"]), y.to(parameters[\"device\"])\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model.forward(x)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            loss = criterion(outputs, y)\n",
        "            acc = (torch.sum(preds == y.data)) / len(x)\n",
        "    \n",
        "            \n",
        "            # Log batch loss and acc\n",
        "            run[f\"fold_{fold}/training/batch/loss\"].log(loss)\n",
        "            run[f\"fold_{fold}/training/batch/acc\"].log(acc)\n",
        "    \n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "    \n",
        "        epoch_acc += torch.sum(preds == y.data).item() \n",
        "        epoch_loss += loss.item() * x.size(0)\n",
        "    epoch_acc_list.append((epoch_acc / len(train_loader.sampler)) * 100)\n",
        "    epoch_loss_list.append(epoch_loss / len(train_loader.sampler))\n",
        "     \n",
        "    # Log model checkpoint       \n",
        "    torch.save(model.state_dict(), f\"./{parameters['model_name']}\")\n",
        "    run[f'fold_{fold}/checkpoint'].upload(parameters['model_name'])\n",
        "    \n",
        "run[\"global/metrics/train/mean_acc\"] = mean(epoch_acc_list)\n",
        "run[\"global/metrics/train/mean_loss\"] = mean(epoch_lost_list)  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://app.neptune.ai/common/showroom/e/SHOW-3378\n",
            "Remember to stop your run once you’ve finished logging your metadata (https://docs.neptune.ai/api-reference/run#stop). It will be stopped automatically only when the notebook kernel/interactive console is terminated.\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90-k5eK205uM"
      },
      "source": [
        "# Basic example"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gXve6tFt1dLd"
      },
      "source": [
        "**Import libraries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n1NVL5h6MlLq",
        "tags": []
      },
      "source": [
        "import neptune.new as neptune\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import ConcatDataset, SubsetRandomSampler, DataLoader\n",
        "from sklearn.model_selection import KFold\n",
        "from statistics import mean"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kfqmkNLB1SRu"
      },
      "source": [
        "## Step 1: Create a Neptune *Run*\n",
        "\n",
        "To log metadata to the Neptune project, you need the `project name` and the `api_token`.\n",
        "\n",
        "To make this example easy to follow, we have created a public project **'common/optuna-integration'** and a shared user **'neptuner'** with the API token **'ANONYMOUS'**. As you will see in the code cell below.\n",
        "\n",
        "**(Optional)** To log to your Neptune project:\n",
        "\n",
        "* [Create a Neptune account](https://app.neptune.ai/register/)\n",
        "\n",
        "* [Find your API token](https://docs.neptune.ai/getting-started/installation#authentication-neptune-api-token)\n",
        "* [Find your project name](https://docs.neptune.ai/getting-started/installation#setting-the-project-name)\n",
        "\n",
        "Pass your credentials to project and api_token arguments of neptune.init()\n",
        "\n",
        "`run = neptune.init(api_token='<YOUR_API_TOKEN>', project='<YOUR_WORKSPACE/YOUR_PROJECT>')` # pass your credentials\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YIAK4NasfQ_f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "219c6e97-2b72-4241-fbe2-aa1200407190"
      },
      "source": [
        "run = neptune.init(\n",
        "    project=\"common/showroom\", tags=\"Colab Notebook\", api_token=\"ANONYMOUS\"\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://app.neptune.ai/common/showroom/e/SHOW-3371\n",
            "Remember to stop your run once you’ve finished logging your metadata (https://docs.neptune.ai/api-reference/run#stop). It will be stopped automatically only when the notebook kernel/interactive console is terminated.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4StPQOmmh_d"
      },
      "source": [
        "Running this cell creates a Run in Neptune, and you can log model building metadata to it.\n",
        "\n",
        "**Click on the link above to open the Run in Neptune UI.** For now, it is empty, but you should keep the tab open to see what happens next"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-WbPjLDk1GcC"
      },
      "source": [
        "## Step 2: Log config and hyperparameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BVJKNQxIYPAC"
      },
      "source": [
        "### Log Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k_8vefGCNBIJ"
      },
      "source": [
        "parameters = {\n",
        "    \"epochs\": 10,\n",
        "    \"lr\": 1e-2,\n",
        "    \"bs\": 10,\n",
        "    \"input_sz\": 32 * 32 * 3,\n",
        "    \"n_classes\": 10,\n",
        "    \"k_folds\": 5,\n",
        "    \"model_name\": \"checkpoint.pth\",\n",
        "    \"device\": torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"),\n",
        "    \"seed\": 42,\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A0Ig4exNSMwE"
      },
      "source": [
        "run[\"global/params\"] = parameters"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P2rAXguq0_IZ"
      },
      "source": [
        "### Log Config\n",
        "Model and Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qwdf0rrklZWi"
      },
      "source": [
        "class BaseModel(nn.Module):\n",
        "    def __init__(self, input_sz, hidden_dim, n_classes):\n",
        "        super(BaseModel, self).__init__()\n",
        "        self.main = nn.Sequential(\n",
        "            nn.Linear(input_sz, hidden_dim * 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim // 2, n_classes),\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        x = input.view(-1, 32 * 32 * 3)\n",
        "        return self.main(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i19Y0g3YGvV2"
      },
      "source": [
        "torch.manual_seed(parameters['seed'])\n",
        "model = BaseModel(\n",
        "    parameters[\"input_sz\"], parameters[\"input_sz\"], parameters[\"n_classes\"]\n",
        ").to(parameters[\"device\"])\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=parameters[\"lr\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "viTM9XYl4-C0"
      },
      "source": [
        "Log model, criterion and optimizer name"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-iZSQE5DYyqX"
      },
      "source": [
        "run[\"global/config/model\"] = type(model).__name__\n",
        "run[\"global/config/criterion\"] = type(criterion).__name__\n",
        "run[\"global/config/optimizer\"] = type(optimizer).__name__"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W2gKRp-8THxa"
      },
      "source": [
        "data_dir = \"data/CIFAR10\"\n",
        "compressed_ds = \"./data/CIFAR10/cifar-10-python.tar.gz\"\n",
        "data_tfms = {\n",
        "    \"train\": transforms.Compose(\n",
        "        [\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
        "        ]\n",
        "    ),\n",
        "    \"val\": transforms.Compose(\n",
        "        [\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
        "        ]\n",
        "    ),\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oMa-mrc64qjz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33361efd-c719-4221-8b1e-d7298e1d3904"
      },
      "source": [
        "trainset = datasets.CIFAR10(data_dir, transform=data_tfms[\"train\"], download=True)\n",
        "\n",
        "validset = datasets.CIFAR10(\n",
        "    data_dir, train=False, transform=data_tfms[\"train\"], download=True\n",
        ")\n",
        "\n",
        "dataset_size = {\"train\": len(trainset), \"val\": len(validset)}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qZY-lkxpIiAc"
      },
      "source": [
        "dataset = ConcatDataset([trainset, validset])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5mXZEVnl4-C1"
      },
      "source": [
        "Log dataset details"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PKFjiUDqgTqT"
      },
      "source": [
        "run[\"global/dataset/CIFAR-10\"].track_files(data_dir)\n",
        "run[\"global/dataset/dataset_transforms\"] = data_tfms\n",
        "run[\"global/dataset/dataset_size\"] = dataset_size"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qRaqN0ug1KP_"
      },
      "source": [
        "## Step 3: Log losses and metrics \n",
        "Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MgGfdruzNB3t"
      },
      "source": [
        "def train_step(run, model,trainloader,loss_fn,optimizer,train=True):\n",
        "    epoch_loss,epoch_acc=0.0,0\n",
        "    if train: \n",
        "        model.train() \n",
        "    else:\n",
        "        model.eval()\n",
        "\n",
        "    for x, y in trainloader:\n",
        "        x, y = x.to(parameters[\"device\"]), y.to(parameters[\"device\"])\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model.forward(x)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        loss = criterion(outputs, y)\n",
        "        acc = (torch.sum(preds == y.data)) / len(x)\n",
        "\n",
        "        if train:\n",
        "            # log batch loss and acc\n",
        "            run[\"training/batch/loss\"].log(loss)\n",
        "            run[\"training/batch/acc\"].log(acc)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        else: \n",
        "            # log batch loss and acc\n",
        "            run[\"validation/batch/loss\"].log(loss)\n",
        "            run[\"validation/batch/acc\"].log(acc)\n",
        "\n",
        "        epoch_acc += torch.sum(preds == y.data).item() \n",
        "        epoch_loss += loss.item() * x.size(0)\n",
        "\n",
        "    epoch_acc = epoch_acc / len(train_loader.sampler)) * 100\n",
        "    epoch_loss = epoch_loss / len(train_loader.sampler))\n",
        "\n",
        "    return epoch_acc, epoch_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AE9tQJWSp916"
      },
      "source": [
        "splits = KFold(n_splits=parameters['k_folds'], shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tECjO3TeMjEY"
      },
      "source": [
        "for fold, (train_idx,val_idx) in enumerate(splits.split(np.arange(len(dataset)))):\n",
        "\n",
        "    train_sampler = SubsetRandomSampler(train_idx)\n",
        "    test_sampler = SubsetRandomSampler(val_idx)\n",
        "    train_loader = DataLoader(dataset, batch_size=parameters['bs'], sampler=train_sampler)\n",
        "    test_loader = DataLoader(dataset, batch_size=parameters['bs'], sampler=test_sampler)\n",
        "\n",
        "    history = {\n",
        "        'train': \n",
        "        {\n",
        "            'mean_loss': [], \n",
        "            'mean_acc': []\n",
        "        }, \n",
        "        'val': \n",
        "        {\n",
        "            'mean_loss': [],\n",
        "            'mean_acc':[]\n",
        "        }\n",
        "    }\n",
        "\n",
        "    for epoch in range(parameters['epochs']):\n",
        "        train_acc, train_loss = train_step(run[f'fold_{fold}'],model,train_loader,criterion,optimizer)\n",
        "        val_acc, val_loss = train_step(run[f'fold_{fold}'],model,test_loader,criterion,optimizer,train=False)\n",
        "\n",
        "        history['train']['mean_loss'].append(train_loss)\n",
        "        history['train']['mean_acc'].append(train_acc)\n",
        "        history['val']['mean_loss'].append(val_loss)\n",
        "        history['val']['mean_acc'].append((val_acc)\n",
        "\n",
        "        # log model weights\n",
        "        torch.save(model.state_dict(), f\"./{parameters['model_name']}\")\n",
        "        run[f'fold_{fold}/checkpoint'].upload(parameters['model_name'])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UsXK-6vaguXi"
      },
      "source": [
        "history['train']['mean_loss'] = mean(history['train']['loss'])\n",
        "history['train']['mean_acc'] = mean(history['train']['acc'])\n",
        "history['val']['mean_loss'] = mean(history['val']['loss'])\n",
        "history['val']['mean_acc'] = mean(history['val']['acc']) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-pC_GC8bXVW2"
      },
      "source": [
        "# log global acc and loss\n",
        "run['global/metrics'] = history"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JhE6C9xv-lXK"
      },
      "source": [
        "# Stop run\n",
        "\n",
        "<font color=red>**Warning:**</font><br>\n",
        "Once you are done logging, you should stop tracking the run using the `stop()` method.\n",
        "This is needed only while logging from a notebook environment. While logging through a script, Neptune automatically stops tracking once the script has completed execution."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Meg02T9p9314",
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2314d4f-e7ca-40ed-a3ff-b89d2047bf68"
      },
      "source": [
        "run.stop()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shutting down background jobs, please wait a moment...\n",
            "Done!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Waiting for the remaining 1 operations to synchronize with Neptune. Do not kill this process.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All 1 operations synced, thanks for waiting!\n"
          ]
        }
      ]
    }
  ]
}