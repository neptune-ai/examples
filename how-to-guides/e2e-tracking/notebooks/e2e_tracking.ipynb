{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to track models end-to-end in Neptune\n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/neptune-ai/examples/blob/main/how-to-guides/e2e-tracking/notebooks/e2e_tracking.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open in Colab\"/>\n",
    "</a>\n",
    "<a target=\"_blank\" href=\"https://github.com/neptune-ai/examples/blob/main/how-to-guides/e2e-tracking\">\n",
    "  <img alt=\"Open in GitHub\" src=\"https://img.shields.io/badge/Open_in_GitHub-blue?logo=github&labelColor=black\">\n",
    "</a>\n",
    "<a target=\"_blank\" href=\"https://app.neptune.ai/o/showcase/org/e2e-tracking/runs/table?viewId=9b71afba-648f-40b7-9c70-98dc99bebc66\"> \n",
    "  <img alt=\"Explore in Neptune\" src=\"https://neptune.ai/wp-content/uploads/2024/01/neptune-badge.svg\">\n",
    "</a> # TODO: UPDATE\n",
    "<a target=\"_blank\" href=\"https://docs.neptune.ai/tutorials/e2e_tracking/\">\n",
    "  <img alt=\"View tutorial in docs\" src=\"https://neptune.ai/wp-content/uploads/2024/01/docs-badge-2.svg\">\n",
    "</a>\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This notebook shows how you can use Neptune to track a model across all stages of its lifecycle by\n",
    "* **Logging model and run metadata to a central project**\n",
    "* **Grouping models by their stage**\n",
    "* **Comparing runs to select the best performing model**\n",
    "* **Monitoring a model once in production**\n",
    "\n",
    "You will learn how to use the Neptune webapp to explore run metadata, compare runs, and manually promote a model. However, this notebook can also be used as a template to design an automated end-to-end pipeline that covers the entire lifecycle of a model  without needing any manual intervention.\n",
    "\n",
    "This example uses Optuna hyperparameter-optimization to simulate training and evaluating multiple XGBoost models, and Evidently to monitor models in production. However, given Neptune's flexibility and [multiple integrations](https://docs.neptune.ai/integrations/), you can use any library and framework of your choice. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before you start\n",
    "\n",
    "This notebook example lets you try out Neptune anonymously, with zero setup.\n",
    "\n",
    "If you want to see the example logged to your own workspace instead:\n",
    "\n",
    "  1. Create a Neptune account. [Register &rarr;](https://neptune.ai/register)\n",
    "  1. Create a Neptune project that you will use for tracking metadata. For instructions, see [Creating a project](https://docs.neptune.ai/setup/creating_project) in the Neptune docs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Neptune and dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -qU \"neptune[xgboost,optuna]\" xgboost scikit-learn optuna evidently matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To fix the random RuntimeError: main thread is not in main loop error in Windows\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.switch_backend(\"agg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Track model training\n",
    "\n",
    "In this section, we'll use the following:\n",
    "1. [Optuna](https://optuna.org/) to train multiple [XGBoost](https://xgboost.readthedocs.io/en/stable/) regression models,\n",
    "2. Neptune's [XGBoost](https://docs.neptune.ai/integrations/xgboost/) and [Optuna](https://docs.neptune.ai/integrations/optuna/) integrations to automatically log metadata and metrics to Neptune for easy run visualization and comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "data, target = fetch_california_housing(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.25)\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dval = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "evals = [(dtrain, \"train\"), (dval, \"valid\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Neptune environment variables\n",
    "\n",
    "To connect to the Neptune app, you need to tell Neptune who you are (`api_token`) and where to send the data (`project`).\n",
    "\n",
    "You can use the default code cell below to create an anonymous run in the public project [common/e2e-tracking](https://app.neptune.ai/common/e2e-tracking). **Note**: Public projects are cleaned regularly, so anonymous runs are only stored temporarily.\n",
    "\n",
    "#### Log to your own project instead\n",
    "\n",
    "Replace the code below with the following:\n",
    "\n",
    "```python\n",
    "import os\n",
    "import neptune\n",
    "from getpass import getpass\n",
    "\n",
    "os.environ[\"NEPTUNE_API_TOKEN\"]=getpass(\"Enter your Neptune API token: \")\n",
    "os.environ[\"NEPTUNE_PROJECT\"]=\"workspace-name/project-name\",  # replace with your own\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import neptune\n",
    "\n",
    "os.environ[\"NEPTUNE_API_TOKEN\"] = neptune.ANONYMOUS_API_TOKEN\n",
    "os.environ[\"NEPTUNE_PROJECT\"] = \"common/e2e-tracking\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Optuna objective function\n",
    "We will create trial level runs within the objective function to capture trial-level metadata using Neptune's XGBoost integration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "code"
    ]
   },
   "outputs": [],
   "source": [
    "from neptune.integrations.xgboost import NeptuneCallback\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    # Define model parameters\n",
    "    model_params = {\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 0, 5),\n",
    "        \"min_child_weight\": trial.suggest_int(\"min_child_weight\", 1, 10),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.1, 0.5),\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.1, 1.0),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.1, 1.0),\n",
    "        \"objective\": \"reg:squarederror\",\n",
    "        \"eval_metric\": [\"mae\", \"rmse\"],\n",
    "    }\n",
    "\n",
    "    # Define training parameters\n",
    "    train_params = {\n",
    "        \"num_boost_round\": trial.suggest_int(\"num_boost_round\", 10, 100),\n",
    "    }\n",
    "\n",
    "    # Create a trial-level run\n",
    "    run_trial_level = neptune.init_run(\n",
    "        capture_hardware_metrics=True,\n",
    "        capture_stderr=True,\n",
    "        capture_stdout=True,\n",
    "        capture_traceback=True,\n",
    "        tags=[\"notebook\", \"trial-level\"],\n",
    "    )\n",
    "\n",
    "    # Log study name and trial number to trial-level run\n",
    "    run_trial_level[\"study-name\"] = str(study.study_name)\n",
    "    run_trial_level[\"trial-number\"] = trial.number\n",
    "\n",
    "    # Log training parameters of a trial-level run\n",
    "    run_trial_level[\"training/parameters\"] = train_params\n",
    "\n",
    "    # Model parameters are logged automatically by the NeptuneCallback\n",
    "\n",
    "    # Create NeptuneCallback to log trial-level metadata\n",
    "    neptune_xgb_callback = NeptuneCallback(run=run_trial_level)\n",
    "\n",
    "    # Train the model and log trial-level metadata to the trial-level run\n",
    "    model = xgb.train(\n",
    "        params=model_params,\n",
    "        dtrain=dtrain,\n",
    "        num_boost_round=train_params[\"num_boost_round\"],\n",
    "        evals=evals,\n",
    "        callbacks=[\n",
    "            neptune_xgb_callback,\n",
    "            xgb.callback.LearningRateScheduler(lambda epoch: 0.99**epoch),\n",
    "            xgb.callback.EarlyStopping(rounds=30, save_best=True, maximize=False),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    # Use group tags to identify the stage of the model\n",
    "    run_trial_level[\"sys/group_tags\"].add([\"development\"])\n",
    "\n",
    "    # Stop trial-level run\n",
    "    run_trial_level.stop()\n",
    "\n",
    "    return model.best_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Optuna study and a Neptune study-level run\n",
    "This run will have all the study-level metadata from Optuna, and can be used to group and compare runs across multiple HPO sweeps/studies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "\n",
    "study = optuna.create_study(direction=\"minimize\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_study_level = neptune.init_run(\n",
    "    capture_hardware_metrics=True,\n",
    "    capture_stderr=True,\n",
    "    capture_stdout=True,\n",
    "    capture_traceback=True,\n",
    "    tags=[\"notebook\", \"study-level\"],\n",
    "    dependencies=\"infer\",\n",
    ")\n",
    "\n",
    "run_study_level[\"study-name\"] = study.study_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Neptune's Optuna callback\n",
    "This will log the HPO sweeps and trials to the study-level run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neptune.integrations.optuna import (\n",
    "    NeptuneCallback as NeptuneOptunaCallback,\n",
    ")  # To avoid name collision\n",
    "\n",
    "neptune_optuna_callback = NeptuneOptunaCallback(run_study_level)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the hyperparameter-sweep with Neptune's Optuna callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study.optimize(\n",
    "    objective,\n",
    "    n_trials=5,\n",
    "    show_progress_bar=True,\n",
    "    callbacks=[neptune_optuna_callback],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop the study level run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_study_level.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare the runs, and choose the best model to move to production\n",
    "\n",
    "You can compare, choose, and promote models both manually using the Neptune web app, or programmatically using the Neptune Python client."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manually through the Neptune web app\n",
    "\n",
    "In this section, we'll:\n",
    "* Explore logged study and trial level metadata using [custom dashboards](https://docs.neptune.ai/app/custom_dashboard/)\n",
    "* Compare trials and sweeps using [custom table views](https://docs.neptune.ai/app/custom_views/)\n",
    "* Select the best model version and [update its stage](https://docs.neptune.ai/model_registry/managing_stage/) to \"production\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üîç Explore the runs \n",
    "* Browse through the logged metadata, images, and charts in the study and trial-level runs.\n",
    "* To view all important metadata in one place, create custom dashboards.\n",
    "\n",
    "You can also browse these example custom dashboards:\n",
    "* [Example study-level custom dashboard](https://app.neptune.ai/o/showcase/org/e2e-tracking/runs/details?viewId=9b71ae0c-8946-4a21-9f1a-4062cad659a4&detailsTab=dashboard&dashboardId=9b71b35b-13fe-4b7d-9d24-a0922d3b07d3&shortId=EET-15&type=run)\n",
    "* [Example trial-level custom dashboard](https://app.neptune.ai/o/showcase/org/e2e-tracking/runs/details?viewId=9b71afba-648f-40b7-9c70-98dc99bebc66&detailsTab=dashboard&dashboardId=9b71b14d-9a28-42a0-ac9e-3fcb0f5985e6&shortId=EET-18&type=run)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ‚öñÔ∏èCompare trials and sweeps\n",
    "\n",
    "* To sort trials, add metrics of importance to the **Experiments** table\n",
    "* Select trials within or across sweeps for more granular comparison\n",
    "\n",
    "You can also browse example custom views:\n",
    "* [Studies sorted by score](https://app.neptune.ai/o/showcase/org/e2e-tracking/runs/table?viewId=9b71ae0c-8946-4a21-9f1a-4062cad659a4&detailsTab=dashboard&dashboardId=Trial-level-9b71b14d-9a28-42a0-ac9e-3fcb0f5985e6&shortId=EET-4&dash=charts&compare=MwGgjOlkA)\n",
    "* [Trials grouped by study](https://app.neptune.ai/o/showcase/org/e2e-tracking/runs/table?viewId=9b71afba-648f-40b7-9c70-98dc99bebc66&detailsTab=dashboard&dashboardId=Trial-level-9b71b14d-9a28-42a0-ac9e-3fcb0f5985e6&shortId=EET-4&dash=charts)\n",
    "\n",
    "and custom compare dashboards:\n",
    "* [Compare studies](https://app.neptune.ai/o/showcase/org/e2e-tracking/runs/compare?viewId=9b71ae0c-8946-4a21-9f1a-4062cad659a4&detailsTab=dashboard&dashboardId=Trial-level-9b71b14d-9a28-42a0-ac9e-3fcb0f5985e6&shortId=EET-4&dash=Compare-sweeps-9b71c0a2-b240-49bb-aa5d-2f0ee85289fa&compare=EwGgbCDsQ)\n",
    "* [Compare trials](https://app.neptune.ai/o/showcase/org/e2e-tracking/runs/compare?viewId=9b71afba-648f-40b7-9c70-98dc99bebc66&detailsTab=dashboard&dashboardId=Trial-level-9b71b14d-9a28-42a0-ac9e-3fcb0f5985e6&shortId=EET-4&dash=Compare-trials-9b71bf85-329a-4855-91dc-c78d96e35079&compare=MwGgjOkQTFehR4g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ü•á Promote the best model to \"production\"\n",
    "\n",
    "* Browse through all the model versions in the **Models** section\n",
    "* Update the stage of the best model to \"production\"\n",
    "\n",
    "You can also browse an [example model registry](https://app.neptune.ai/o/showcase/org/e2e-tracking/models?shortId=EET-RFR&type=model) *(read-only)*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select the best model programmatically\n",
    "\n",
    "All the comparisons and selections done manually above can also be automated programmatically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download the model versions table as a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_versions_df = npt_model.fetch_model_versions_table(\n",
    "    columns=[\"sys/stage\", \"training/score\"],\n",
    "    sort_by=\"training/score\",\n",
    "    ascending=True,\n",
    "    progress_bar=False,\n",
    ").to_pandas()\n",
    "\n",
    "model_versions_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get scores and IDs of challenger and champion models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    champion_model = model_versions_df[model_versions_df[\"sys/stage\"] == \"production\"][\n",
    "        \"sys/id\"\n",
    "    ].values[0]\n",
    "    champion_model_score = model_versions_df[model_versions_df[\"sys/stage\"] == \"production\"][\n",
    "        \"training/score\"\n",
    "    ].values[0]\n",
    "    print(f\"Champion model ID: {champion_model} and score: {champion_model_score}\")\n",
    "    NO_CHAMPION = False\n",
    "except IndexError:\n",
    "    print(\"‚ùå No model found in production\")\n",
    "    NO_CHAMPION = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "staged_models = model_versions_df[model_versions_df[\"sys/stage\"] == \"staging\"]\n",
    "challenger_model_score = min(staged_models[\"training/score\"])\n",
    "challenger_model_id = staged_models[staged_models[\"training/score\"] == challenger_model_score][\n",
    "    \"sys/id\"\n",
    "].values[0]\n",
    "\n",
    "print(f\"Challenger model ID: {challenger_model_id} and score: {challenger_model_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Promote challenger to champion if score is better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if NO_CHAMPION:\n",
    "    print(f\"Promoting {challenger_model_id} to Production\")\n",
    "    with neptune.init_model_version(with_id=challenger_model_id) as challenger_model:\n",
    "        challenger_model.change_stage(\"production\")\n",
    "\n",
    "elif challenger_model_score < champion_model_score:\n",
    "    print(\"Challenger is better than champion\")\n",
    "\n",
    "    print(f\"Archiving champion model {champion_model}\")\n",
    "    with neptune.init_model_version(with_id=champion_model) as champion_model:\n",
    "        champion_model.change_stage(\"archived\")\n",
    "\n",
    "    print(f\"Promoting {challenger_model_id} to Production\")\n",
    "    with neptune.init_model_version(with_id=challenger_model_id) as challenger_model:\n",
    "        challenger_model.change_stage(\"production\")\n",
    "\n",
    "else:\n",
    "    print(\"Champion model is better than challenger\")\n",
    "    print(f\"Archiving challenger model {challenger_model_id}\")\n",
    "    with neptune.init_model_version(with_id=challenger_model_id) as challenger_model:\n",
    "        challenger_model.change_stage(\"archived\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitor model in production\n",
    "In this section, we'll:\n",
    "1. Download the model binary from the model registry to make predictions in production\n",
    "2. Use [EvidentlyAI](https://www.evidentlyai.com/) to monitor the model in production.\n",
    "\n",
    "We'll use a modified version of a tutorial from the [Evidently documentation](https://docs.evidentlyai.com/get-started/tutorial)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup\n",
    "\n",
    "We'll use an example dataset and mock historical predictions to use as a reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "from evidently.report import Report\n",
    "from evidently.metric_preset import RegressionPreset\n",
    "from evidently.metrics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = fetch_california_housing(as_frame=True)\n",
    "housing_data = data.frame\n",
    "\n",
    "housing_data.rename(columns={\"MedHouseVal\": \"target\"}, inplace=True)\n",
    "\n",
    "reference = housing_data.sample(n=10000, replace=False)\n",
    "reference[\"prediction\"] = reference[\"target\"].values + np.random.normal(0, 5, reference.shape[0])\n",
    "\n",
    "current = housing_data.sample(n=10000, replace=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download saved model from model registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_versions_df = npt_model.fetch_model_versions_table(\n",
    "    columns=[\"sys/stage\"], progress_bar=False\n",
    ").to_pandas()\n",
    "\n",
    "production_model = model_versions_df[model_versions_df[\"sys/stage\"] == \"production\"][\n",
    "    \"sys/id\"\n",
    "].values[0]\n",
    "\n",
    "npt_model_version = neptune.init_model_version(with_id=production_model)\n",
    "npt_model_version[\"saved_model\"].download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make predictions from downloaded model on current test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "\n",
    "with open(\"saved_model.pkl\", \"rb\") as f:\n",
    "    model = pkl.load(f)\n",
    "\n",
    "current[\"prediction\"] = model.predict(current.drop(columns=[\"target\"]))\n",
    "\n",
    "current"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate regression report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_performance_report = Report(metrics=[RegressionPreset()])\n",
    "\n",
    "reg_performance_report.run(reference_data=reference, current_data=current)\n",
    "\n",
    "reg_performance_report.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload report to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_performance_report.save_html(\"report.html\")\n",
    "npt_model_version[\"production/report\"].upload(\"report.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload metrics to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neptune.utils import stringify_unsupported\n",
    "\n",
    "npt_model_version[\"production/metrics\"] = stringify_unsupported(\n",
    "    reg_performance_report.as_dict()[\"metrics\"][0]\n",
    ")\n",
    "npt_model_version.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These metrics can then be fetched downstream to trigger a model refresh or retraining, if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retraining_threshold = 0.5\n",
    "\n",
    "if (\n",
    "    npt_model_version[\"production/metrics/result/current/mean_abs_error\"].fetch()\n",
    "    > retraining_threshold\n",
    "):\n",
    "    print(\"Model degradation detected. Retraining model...\")\n",
    "    ...\n",
    "else:\n",
    "    print(\"Model performance within expectations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop Neptune objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project.stop()\n",
    "npt_model.stop()\n",
    "npt_model_version.stop()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Neptune_E2E_Model_Tracking.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "vscode": {
   "interpreter": {
    "hash": "a9715cf0b0024f6e1c62cb31a4f1f43970eb41991212681878768b4bfe53050a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
