{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to track models end-to-end in Neptune\n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/neptune-ai/examples/blob/main/how-to-guides/e2e-tracking/notebooks/e2e_tracking.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open in Colab\"/>\n",
    "</a>\n",
    "<a target=\"_blank\" href=\"https://github.com/neptune-ai/examples/blob/main/how-to-guides/e2e-tracking\">\n",
    "  <img alt=\"Open in GitHub\" src=\"https://img.shields.io/badge/Open_in_GitHub-blue?logo=github&labelColor=black\">\n",
    "</a>\n",
    "<a target=\"_blank\" href=\"https://app.neptune.ai/o/showcase/org/e2e-tracking/runs/table?viewId=9b71afba-648f-40b7-9c70-98dc99bebc66\"> \n",
    "  <img alt=\"Explore in Neptune\" src=\"https://neptune.ai/wp-content/uploads/2024/01/neptune-badge.svg\">\n",
    "</a>\n",
    "<a target=\"_blank\" href=\"https://docs.neptune.ai/tutorials/e2e_tracking/\">\n",
    "  <img alt=\"View tutorial in docs\" src=\"https://neptune.ai/wp-content/uploads/2024/01/docs-badge-2.svg\">\n",
    "</a>\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This notebook shows how you can use Neptune to track a model across all stages of its lifecycle by\n",
    "* **Logging model and run metadata to a central project**\n",
    "* **Comparing runs to select the best performing model**\n",
    "* **Monitoring a model once in production**\n",
    "\n",
    "You will learn how to use the Neptune webapp to explore run metadata, compare runs, and manually promote a model. However, this notebook can also be used as a template to design an automated end-to-end pipeline that covers the entire lifecycle of a model  without needing any manual intervention.\n",
    "\n",
    "This example uses Optuna hyperparameter-optimization to simulate training and evaluating multiple scikit-learn models, and Evidently to monitor models in production. However, given Neptune's flexibility and [multiple integrations](https://docs.neptune.ai/integrations/), you can use any library and framework of your choice. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before you start\n",
    "\n",
    "This notebook example lets you try out Neptune anonymously, with zero setup.\n",
    "\n",
    "If you want to see the example logged to your own workspace instead:\n",
    "\n",
    "  1. Create a Neptune account. [Register &rarr;](https://neptune.ai/register)\n",
    "  1. Create a Neptune project that you will use for tracking metadata. For instructions, see [Creating a project](https://docs.neptune.ai/setup/creating_project) in the Neptune docs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Neptune and dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q -U neptune[sklearn,optuna] scikit-learn optuna evidently matplotlib scipy<1.12\n",
    "%pip install -q --user scikit-learn matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this if you get a `RuntimeError: main thread is not in main loop` error\n",
    "import matplotlib\n",
    "\n",
    "matplotlib.use(\"Agg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Track model training\n",
    "\n",
    "In this section, we'll use the following:\n",
    "1. Optuna to train multiple scikit-learn regression models,\n",
    "2. Neptune's [scikit-learn](https://docs.neptune.ai/integrations/sklearn/) and [Optuna](https://docs.neptune.ai/integrations/optuna/) integrations to automatically log metadata and metrics to Neptune for easy run comparison,\n",
    "3. Neptune's [model registry](https://docs.neptune.ai/model_registry/overview/) to track models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "data, target = fetch_california_housing(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the Neptune project\n",
    "\n",
    "To connect to the Neptune app, you need to tell Neptune who you are (`api_token`) and where to send the data (`project`).\n",
    "\n",
    "You can use the default code cell below to create an anonymous run in the public project [common/e2e-tracking](https://app.neptune.ai/common/e2e-tracking). **Note**: Public projects are cleaned regularly, so anonymous runs are only stored temporarily.\n",
    "\n",
    "#### Log to your own project instead\n",
    "\n",
    "Replace the code below with the following:\n",
    "\n",
    "```python\n",
    "import os\n",
    "import neptune\n",
    "from getpass import getpass\n",
    "\n",
    "os.environ[\"NEPTUNE_API_TOKEN\"]=getpass(\"Enter your Neptune API token: \")\n",
    "os.environ[\"NEPTUNE_PROJECT\"]=\"workspace-name/project-name\",  # replace with your own\n",
    "\n",
    "project = neptune.init_project()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import neptune\n",
    "\n",
    "os.environ[\"NEPTUNE_API_TOKEN\"] = neptune.ANONYMOUS_API_TOKEN\n",
    "os.environ[\"NEPTUNE_PROJECT\"] = \"common/e2e-tracking\"\n",
    "\n",
    "project = neptune.init_project(mode=\"read-only\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a new model in the model registry\n",
    "This model will serve as a placeholder for all the model versions created in different Optuna trials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neptune.exceptions import NeptuneModelKeyAlreadyExistsError\n",
    "\n",
    "model_key = \"RFR\"\n",
    "\n",
    "try:\n",
    "    # Create a new model if it does not already exist\n",
    "    npt_model = neptune.init_model(key=model_key)\n",
    "except NeptuneModelKeyAlreadyExistsError:\n",
    "    # Initialize the model if it already exists\n",
    "    npt_model = neptune.init_model(with_id=f\"{project['sys/id'].fetch()}-{model_key}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Optuna objective function\n",
    "We will create trial level runs and model versions within the objective function to capture trial-level metadata using Neptune's scikit-learn integration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "code"
    ]
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from neptune.integrations.sklearn import create_regressor_summary, get_pickled_model\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    param = {\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 2, 64),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 2, 5),\n",
    "        \"min_samples_split\": trial.suggest_int(\"min_samples_split\", 3, 10),\n",
    "    }\n",
    "\n",
    "    # Create a trial-level run\n",
    "    run_trial_level = neptune.init_run(\n",
    "        capture_hardware_metrics=True,\n",
    "        capture_stderr=True,\n",
    "        capture_stdout=True,\n",
    "        tags=[\"notebook\", \"trial-level\"],\n",
    "    )\n",
    "\n",
    "    # Log study name and trial number to trial-level run\n",
    "    run_trial_level[\"study-name\"] = str(study.study_name)\n",
    "    run_trial_level[\"trial-number\"] = trial.number\n",
    "\n",
    "    # Log parameters of a trial-level run\n",
    "    run_trial_level[\"parameters\"] = param\n",
    "\n",
    "    # Train the model\n",
    "    model = RandomForestRegressor(**param)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Log model metadata to the trial level run\n",
    "    run_trial_level[\"model_summary\"] = create_regressor_summary(\n",
    "        model, X_train, X_test, y_train, y_test\n",
    "    )\n",
    "\n",
    "    # Fetch objective score from the run\n",
    "    run_trial_level.wait()\n",
    "    score = run_trial_level[\"model_summary/test/scores/mean_absolute_error\"].fetch()\n",
    "\n",
    "    # Create a new model version\n",
    "    model_version = neptune.init_model_version(model=f\"{project['sys/id'].fetch()}-{model_key}\")\n",
    "\n",
    "    # Link model-version to the trial-level run\n",
    "    model_version[\"training/run/id\"] = run_trial_level[\"sys/id\"].fetch()\n",
    "    model_version[\"training/run/url\"] = run_trial_level.get_url()\n",
    "\n",
    "    run_trial_level[\"model_summary/id\"] = model_version[\"sys/id\"].fetch()\n",
    "    run_trial_level[\"model_summary/url\"] = model_version.get_url()\n",
    "\n",
    "    # Log score to model version\n",
    "    model_version[\"training/score\"] = score\n",
    "\n",
    "    # Upload model binary to model version\n",
    "    model_version[\"saved_model\"].upload(get_pickled_model(model))\n",
    "\n",
    "    # Update model stage to \"staging\"\n",
    "    model_version.change_stage(\"staging\")\n",
    "\n",
    "    # Stop model version and trial-level run\n",
    "    model_version.stop()\n",
    "    run_trial_level.stop()\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Optuna study and a Neptune study-level run\n",
    "This run will have all the study-level metadata from Optuna, and can be used to group and compare runs across multiple HPO sweeps/studies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "\n",
    "study = optuna.create_study(direction=\"minimize\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_study_level = neptune.init_run(\n",
    "    capture_hardware_metrics=True,\n",
    "    capture_stderr=True,\n",
    "    capture_stdout=True,\n",
    "    tags=[\"notebook\", \"study-level\"],\n",
    "    dependencies=\"infer\",\n",
    ")\n",
    "\n",
    "run_study_level[\"study-name\"] = study.study_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Neptune's Optuna callback\n",
    "This will log the HPO sweeps and trials to the study-level run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neptune.integrations.optuna import NeptuneCallback\n",
    "\n",
    "neptune_optuna_callback = NeptuneCallback(run_study_level)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the hyperparameter-sweep with Neptune's Optuna callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study.optimize(objective, n_trials=5, callbacks=[neptune_optuna_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop the study level run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_study_level.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare the runs, and choose the best model to move to production\n",
    "\n",
    "You can compare, choose, and promote models both manually using the Neptune web app, or programmatically using the Neptune Python client."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manually through the Neptune web app\n",
    "\n",
    "In this section, we'll:\n",
    "* Explore logged study and trial level metadata using [custom dashboards](https://docs.neptune.ai/app/custom_dashboard/)\n",
    "* Compare trials and sweeps using [custom table views](https://docs.neptune.ai/app/custom_views/)\n",
    "* Select the best model version and [update its stage](https://docs.neptune.ai/model_registry/managing_stage/) to \"production\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 🔍 Explore the runs \n",
    "* Browse through the logged metadata, images, and charts in the study and trial-level runs.\n",
    "* To view all important metadata in one place, create custom dashboards.\n",
    "\n",
    "You can also browse these example custom dashboards:\n",
    "* [Example study-level custom dashboard](https://app.neptune.ai/o/showcase/org/e2e-tracking/runs/details?viewId=9b71ae0c-8946-4a21-9f1a-4062cad659a4&detailsTab=dashboard&dashboardId=9b71b35b-13fe-4b7d-9d24-a0922d3b07d3&shortId=EET-15&type=run)\n",
    "* [Example trial-level custom dashboard](https://app.neptune.ai/o/showcase/org/e2e-tracking/runs/details?viewId=9b71afba-648f-40b7-9c70-98dc99bebc66&detailsTab=dashboard&dashboardId=9b71b14d-9a28-42a0-ac9e-3fcb0f5985e6&shortId=EET-18&type=run)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ⚖️Compare trials and sweeps\n",
    "\n",
    "* To sort trials, add metrics of importance to the **Experiments** table\n",
    "* Select trials within or across sweeps for more granular comparison\n",
    "\n",
    "You can also browse example custom views:\n",
    "* [Studies sorted by score](https://app.neptune.ai/o/showcase/org/e2e-tracking/runs/table?viewId=9b71ae0c-8946-4a21-9f1a-4062cad659a4&detailsTab=dashboard&dashboardId=Trial-level-9b71b14d-9a28-42a0-ac9e-3fcb0f5985e6&shortId=EET-4&dash=charts&compare=MwGgjOlkA)\n",
    "* [Trials grouped by study](https://app.neptune.ai/o/showcase/org/e2e-tracking/runs/table?viewId=9b71afba-648f-40b7-9c70-98dc99bebc66&detailsTab=dashboard&dashboardId=Trial-level-9b71b14d-9a28-42a0-ac9e-3fcb0f5985e6&shortId=EET-4&dash=charts)\n",
    "\n",
    "and custom compare dashboards:\n",
    "* [Compare studies](https://app.neptune.ai/o/showcase/org/e2e-tracking/runs/compare?viewId=9b71ae0c-8946-4a21-9f1a-4062cad659a4&detailsTab=dashboard&dashboardId=Trial-level-9b71b14d-9a28-42a0-ac9e-3fcb0f5985e6&shortId=EET-4&dash=Compare-sweeps-9b71c0a2-b240-49bb-aa5d-2f0ee85289fa&compare=EwGgbCDsQ)\n",
    "* [Compare trials](https://app.neptune.ai/o/showcase/org/e2e-tracking/runs/compare?viewId=9b71afba-648f-40b7-9c70-98dc99bebc66&detailsTab=dashboard&dashboardId=Trial-level-9b71b14d-9a28-42a0-ac9e-3fcb0f5985e6&shortId=EET-4&dash=Compare-trials-9b71bf85-329a-4855-91dc-c78d96e35079&compare=MwGgjOkQTFehR4g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 🥇 Promote the best model to \"production\"\n",
    "\n",
    "* Browse through all the model versions in the **Models** section\n",
    "* Update the stage of the best model to \"production\"\n",
    "\n",
    "You can also browse an [example model registry](https://app.neptune.ai/o/showcase/org/e2e-tracking/models?shortId=EET-RFR&type=model) *(read-only)*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select the best model programmatically\n",
    "\n",
    "All the comparisons and selections done manually above can also be automated programmatically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download the model versions table as a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_versions_df = npt_model.fetch_model_versions_table(\n",
    "    columns=[\"sys/stage\", \"training/score\"],\n",
    "    sort_by=\"training/score\",\n",
    "    ascending=True,\n",
    "    progress_bar=False,\n",
    ").to_pandas()\n",
    "\n",
    "model_versions_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get scores and IDs of challenger and champion models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    champion_model = model_versions_df[model_versions_df[\"sys/stage\"] == \"production\"][\n",
    "        \"sys/id\"\n",
    "    ].values[0]\n",
    "    champion_model_score = model_versions_df[model_versions_df[\"sys/stage\"] == \"production\"][\n",
    "        \"training/score\"\n",
    "    ].values[0]\n",
    "    print(f\"Champion model ID: {champion_model} and score: {champion_model_score}\")\n",
    "    NO_CHAMPION = False\n",
    "except IndexError:\n",
    "    print(\"❌ No model found in production\")\n",
    "    NO_CHAMPION = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "staged_models = model_versions_df[model_versions_df[\"sys/stage\"] == \"staging\"]\n",
    "challenger_model_score = min(staged_models[\"training/score\"])\n",
    "challenger_model_id = staged_models[staged_models[\"training/score\"] == challenger_model_score][\n",
    "    \"sys/id\"\n",
    "].values[0]\n",
    "\n",
    "print(f\"Challenger model ID: {challenger_model_id} and score: {challenger_model_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Promote challenger to champion if score is better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if NO_CHAMPION:\n",
    "    print(f\"Promoting {challenger_model_id} to Production\")\n",
    "    with neptune.init_model_version(with_id=challenger_model_id) as challenger_model:\n",
    "        challenger_model.change_stage(\"production\")\n",
    "\n",
    "elif challenger_model_score < champion_model_score:\n",
    "    print(\"Challenger is better than champion\")\n",
    "\n",
    "    print(f\"Archiving champion model {champion_model}\")\n",
    "    with neptune.init_model_version(with_id=champion_model) as champion_model:\n",
    "        champion_model.change_stage(\"archived\")\n",
    "\n",
    "    print(f\"Promoting {challenger_model_id} to Production\")\n",
    "    with neptune.init_model_version(with_id=challenger_model_id) as challenger_model:\n",
    "        challenger_model.change_stage(\"production\")\n",
    "\n",
    "else:\n",
    "    print(\"Champion model is better than challenger\")\n",
    "    print(f\"Archiving challenger model {challenger_model_id}\")\n",
    "    with neptune.init_model_version(with_id=challenger_model_id) as challenger_model:\n",
    "        challenger_model.change_stage(\"archived\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitor model in production\n",
    "In this section, we'll:\n",
    "1. Download the model binary from the model registry to make predictions in production\n",
    "2. Use [EvidentlyAI](https://www.evidentlyai.com/) to monitor the model in production.\n",
    "\n",
    "We'll use a modified version of a tutorial from the [Evidently documentation](https://docs.evidentlyai.com/get-started/tutorial)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup\n",
    "\n",
    "We'll use an example dataset and mock historical predictions to use as a reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "from evidently.report import Report\n",
    "from evidently.metric_preset import RegressionPreset\n",
    "from evidently.metrics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = fetch_california_housing(as_frame=True)\n",
    "housing_data = data.frame\n",
    "\n",
    "housing_data.rename(columns={\"MedHouseVal\": \"target\"}, inplace=True)\n",
    "\n",
    "reference = housing_data.sample(n=10000, replace=False)\n",
    "reference[\"prediction\"] = reference[\"target\"].values + np.random.normal(0, 5, reference.shape[0])\n",
    "\n",
    "current = housing_data.sample(n=10000, replace=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download saved model from model registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_versions_df = npt_model.fetch_model_versions_table(\n",
    "    columns=[\"sys/stage\"], progress_bar=False\n",
    ").to_pandas()\n",
    "\n",
    "production_model = model_versions_df[model_versions_df[\"sys/stage\"] == \"production\"][\n",
    "    \"sys/id\"\n",
    "].values[0]\n",
    "\n",
    "npt_model_version = neptune.init_model_version(with_id=production_model)\n",
    "npt_model_version[\"saved_model\"].download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make predictions from downloaded model on current test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "\n",
    "with open(\"saved_model.pkl\", \"rb\") as f:\n",
    "    model = pkl.load(f)\n",
    "\n",
    "current[\"prediction\"] = model.predict(current.drop(columns=[\"target\"]))\n",
    "\n",
    "current"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate regression report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_performance_report = Report(metrics=[RegressionPreset()])\n",
    "\n",
    "reg_performance_report.run(reference_data=reference, current_data=current)\n",
    "\n",
    "reg_performance_report.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload report to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_performance_report.save_html(\"report.html\")\n",
    "npt_model_version[\"production/report\"].upload(\"report.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload metrics to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neptune.utils import stringify_unsupported\n",
    "\n",
    "npt_model_version[\"production/metrics\"] = stringify_unsupported(\n",
    "    reg_performance_report.as_dict()[\"metrics\"][0]\n",
    ")\n",
    "npt_model_version.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These metrics can then be fetched downstream to trigger a model refresh or retraining, if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retraining_threshold = 0.5\n",
    "\n",
    "if (\n",
    "    npt_model_version[\"production/metrics/result/current/mean_abs_error\"].fetch()\n",
    "    > retraining_threshold\n",
    "):\n",
    "    print(\"Model degradation detected. Retraining model...\")\n",
    "    ...\n",
    "else:\n",
    "    print(\"Model performance within expectations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop Neptune objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project.stop()\n",
    "npt_model.stop()\n",
    "npt_model_version.stop()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Neptune_E2E_Model_Tracking.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "vscode": {
   "interpreter": {
    "hash": "a9715cf0b0024f6e1c62cb31a4f1f43970eb41991212681878768b4bfe53050a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
