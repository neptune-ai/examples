{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to track models end-to-end in Neptune\n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/neptune-ai/examples/blob/main/how-to-guides/e2e-tracking/notebooks/e2e_tracking.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open in Colab\"/>\n",
    "</a>\n",
    "<a target=\"_blank\" href=\"https://github.com/neptune-ai/examples/blob/main/how-to-guides/e2e-tracking\">\n",
    "  <img alt=\"Open in GitHub\" src=\"https://img.shields.io/badge/Open_in_GitHub-blue?logo=github&labelColor=black\">\n",
    "</a>\n",
    "<a target=\"_blank\" href=\"https://app.neptune.ai/o/showcase/org/e2e-tracking/runs/table?viewId=9cb5bc7c-3bce-4c69-8f5c-90d3d9cc682c\"> \n",
    "  <img alt=\"Explore in Neptune\" src=\"https://neptune.ai/wp-content/uploads/2024/01/neptune-badge.svg\">\n",
    "</a>\n",
    "<a target=\"_blank\" href=\"https://docs-legacy.neptune.ai/tutorials/e2e_tracking/\">\n",
    "  <img alt=\"View tutorial in docs\" src=\"https://neptune.ai/wp-content/uploads/2024/01/docs-badge-2.svg\">\n",
    "</a>\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This notebook shows how you can use Neptune to track a model across all stages of its lifecycle by\n",
    "* **Logging model and run metadata to a central project**\n",
    "* **Grouping models by their stage**\n",
    "* **Comparing models to select the best performing model**\n",
    "* **Monitoring a model once in production**\n",
    "\n",
    "You will learn how to use the Neptune webapp to explore run metadata, compare runs, and manually promote a model. However, this notebook can also be used as a template to design an automated end-to-end pipeline that covers the entire lifecycle of a model  without needing any manual intervention.\n",
    "\n",
    "This example uses Optuna hyperparameter-optimization to simulate training and evaluating multiple XGBoost models, and Evidently to monitor models in production. However, given Neptune's flexibility and [multiple integrations](https://docs-legacy.neptune.ai/integrations/), you can use any library and framework of your choice. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before you start\n",
    "\n",
    "This notebook example lets you try out Neptune anonymously, with zero setup.\n",
    "\n",
    "If you want to see the example logged to your own workspace instead:\n",
    "\n",
    "  1. Create a Neptune account. [Register &rarr;](https://neptune.ai/register)\n",
    "  1. Create a Neptune project that you will use for tracking metadata. For instructions, see [Creating a project](https://docs-legacy.neptune.ai/setup/creating_project) in the Neptune docs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Neptune and dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -qU \"neptune[xgboost,optuna]\" xgboost scikit-learn optuna evidently matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To fix the `RuntimeError: main thread is not in main loop` error in Windows\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.switch_backend(\"agg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Track model training\n",
    "\n",
    "In this section, we'll use the following:\n",
    "- [Optuna](https://optuna.org/) to train multiple [XGBoost](https://xgboost.readthedocs.io/en/stable/) regression models,\n",
    "- Neptune's [XGBoost](https://docs-legacy.neptune.ai/integrations/xgboost/) and [Optuna](hdocs-legacy.neptune.aiacy.neptune.ai/integrations/optuna/) integrations to automatically log metadata and metrics to Neptune for easy run visualization and comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "data, target = fetch_california_housing(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.25)\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dval = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "evals = [(dtrain, \"train\"), (dval, \"valid\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up Neptune environment variables\n",
    "\n",
    "To connect to the Neptune app, you need to tell Neptune who you are (`api_token`) and where to send the data (`project`).\n",
    "\n",
    "You can use the default code cell below to create an anonymous run in the public project [common/e2e-tracking](https://app.neptune.ai/common/e2e-tracking). **Note**: Public projects are cleaned regularly, so anonymous runs are only stored temporarily.\n",
    "\n",
    "#### Log to your own project instead\n",
    "\n",
    "Replace the code below with the following:\n",
    "\n",
    "```python\n",
    "import os\n",
    "import neptune\n",
    "from getpass import getpass\n",
    "\n",
    "os.environ[\"NEPTUNE_API_TOKEN\"]=getpass(\"Enter your Neptune API token: \")\n",
    "os.environ[\"NEPTUNE_PROJECT\"]=\"workspace-name/project-name\",  # replace with your own\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import neptune\n",
    "\n",
    "os.environ[\"NEPTUNE_API_TOKEN\"] = neptune.ANONYMOUS_API_TOKEN\n",
    "os.environ[\"NEPTUNE_PROJECT\"] = \"common/e2e-tracking\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Optuna objective function\n",
    "We will create trial level runs within the objective function to capture trial-level metadata using Neptune's XGBoost integration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "code"
    ]
   },
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    from neptune.integrations.xgboost import NeptuneCallback\n",
    "\n",
    "    # Define model parameters\n",
    "    model_params = {\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 0, 10),\n",
    "        \"min_child_weight\": trial.suggest_int(\"min_child_weight\", 2, 9),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.5, 1.0),\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 0.75),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.4, 1.0),\n",
    "        \"objective\": \"reg:squarederror\",\n",
    "        \"eval_metric\": [\"mae\", \"rmse\"],\n",
    "    }\n",
    "\n",
    "    # Define training parameters\n",
    "    train_params = {\n",
    "        \"num_boost_round\": trial.suggest_int(\"num_boost_round\", 10, 50),\n",
    "    }\n",
    "\n",
    "    # Create a trial-level run\n",
    "    run_trial_level = neptune.init_run(\n",
    "        capture_hardware_metrics=True,\n",
    "        capture_stderr=True,\n",
    "        capture_stdout=True,\n",
    "        capture_traceback=True,\n",
    "        tags=[\"notebook\", \"trial-level\"],\n",
    "    )\n",
    "\n",
    "    # Log study name and trial number to trial-level run\n",
    "    run_trial_level[\"study-name\"] = str(study.study_name)\n",
    "    run_trial_level[\"trial-number\"] = trial.number\n",
    "\n",
    "    # Log training parameters of a trial-level run\n",
    "    run_trial_level[\"training/parameters\"] = train_params\n",
    "\n",
    "    # Model parameters are logged automatically by the NeptuneCallback\n",
    "\n",
    "    # Create NeptuneCallback to log trial-level metadata\n",
    "    neptune_xgb_callback = NeptuneCallback(run=run_trial_level)\n",
    "\n",
    "    # Train the model and log trial-level metadata to the trial-level run\n",
    "    model = xgb.train(\n",
    "        params=model_params,\n",
    "        dtrain=dtrain,\n",
    "        num_boost_round=train_params[\"num_boost_round\"],\n",
    "        evals=evals,\n",
    "        verbose_eval=False,\n",
    "        callbacks=[\n",
    "            neptune_xgb_callback,\n",
    "            xgb.callback.LearningRateScheduler(lambda epoch: 0.99**epoch),\n",
    "            xgb.callback.EarlyStopping(rounds=10, save_best=True, maximize=False),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    # Use group tags to identify the stage of the model\n",
    "    run_trial_level[\"sys/group_tags\"].add([\"development\"])\n",
    "\n",
    "    # Stop trial-level run\n",
    "    run_trial_level.stop()\n",
    "\n",
    "    return model.best_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Optuna study and a Neptune study-level run\n",
    "This run will have all the study-level metadata from Optuna, and can be used to group and compare runs across multiple HPO sweeps/studies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "\n",
    "study = optuna.create_study(direction=\"minimize\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_study_level = neptune.init_run(\n",
    "    capture_hardware_metrics=True,\n",
    "    capture_stderr=True,\n",
    "    capture_stdout=True,\n",
    "    capture_traceback=True,\n",
    "    tags=[\"notebook\", \"study-level\"],\n",
    "    dependencies=\"infer\",\n",
    ")\n",
    "\n",
    "run_study_level[\"study-name\"] = study.study_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Neptune's Optuna callback\n",
    "This will log the HPO sweeps and trials to the study-level run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neptune.integrations.optuna import NeptuneCallback as NeptuneOptunaCallback\n",
    "\n",
    "neptune_optuna_callback = NeptuneOptunaCallback(run_study_level)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the hyperparameter-sweep with Neptune's Optuna callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study.optimize(\n",
    "    objective,\n",
    "    n_trials=5,\n",
    "    show_progress_bar=True,\n",
    "    callbacks=[neptune_optuna_callback],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop the study level run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_study_level.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare the runs, and choose the best model to move to production\n",
    "\n",
    "You can compare, choose, and promote models both manually using the Neptune web app, or programmatically using the Neptune Python client."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manually through the Neptune web app\n",
    "\n",
    "In this section, we'll:\n",
    "* Explore logged study and trial level metadata using [custom dashboards](https://docs-legacy.neptune.ai/app/custom_dashboard/)\n",
    "* Compare trials and sweeps using [custom table views](https://docs-legacy.neptune.ai/app/custom_views/)\n",
    "* Select the best model version and update its stage to *production*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üîç Explore the runs \n",
    "* Browse through the logged metadata, images, and charts in the study and trial-level runs.\n",
    "* To view all important metadata in one place, create custom dashboards.\n",
    "\n",
    "You can also browse these example custom dashboards:\n",
    "* [Example study-level custom dashboard](https://app.neptune.ai/o/showcase/org/e2e-tracking/runs/details?viewId=9b71ae0c-8946-4a21-9f1a-4062cad659a4&detailsTab=dashboard&dashboardId=9b71b35b-13fe-4b7d-9d24-a0922d3b07d3&shortId=EET-34&type=run)\n",
    "* [Example trial-level custom dashboard](https://app.neptune.ai/o/showcase/org/e2e-tracking/runs/details?viewId=9cb5bc7c-3bce-4c69-8f5c-90d3d9cc682c&detailsTab=dashboard&dashboardId=9cb5c2b5-d0ab-4760-8c96-2e3116db1089&shortId=EET-36&type=run) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ‚öñÔ∏è Compare models\n",
    "\n",
    "* To sort models, add important metrics to the **Experiments** table\n",
    "* Select models within or across studies for more granular comparison\n",
    "\n",
    "You can also browse an example custom table view with [models grouped by stage and sorted by score](https://app.neptune.ai/o/showcase/org/e2e-tracking/runs/table?viewId=9cb5bc7c-3bce-4c69-8f5c-90d3d9cc682c) and custom dashboard [comparing models across stages](https://app.neptune.ai/o/showcase/org/e2e-tracking/runs/compare?viewId=9cb5bc7c-3bce-4c69-8f5c-90d3d9cc682c&dash=dashboard&dashboardId=Compare-models-9cb5bf5a-32c9-4def-addc-0fb8bb1a9ce3&compare=EwTgNAjJ1cP3KTJA)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ü•á Promote the best model to \"production\"\n",
    "\n",
    "* Browse through all the models\n",
    "* Update the stage of the best model to \"production\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select the best model programmatically\n",
    "\n",
    "All the comparisons and selections done manually above can also be automated programmatically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download the runs table as a pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_namespace = (\n",
    "    \"training/early_stopping/best_score\"  # This is where the best score is logged in Neptune\n",
    ")\n",
    "\n",
    "project = neptune.init_project(mode=\"read-only\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fetch the *champion* model. This is the model currently in production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "champion_model_df = project.fetch_runs_table(\n",
    "    query='`sys/group_tags`:stringSet CONTAINS \"production\"',\n",
    "    columns=[score_namespace],\n",
    "    sort_by=score_namespace,\n",
    "    ascending=True,\n",
    "    limit=1,\n",
    ").to_pandas()\n",
    "\n",
    "champion_model_df if not champion_model_df.empty else print(\"No champion model found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fetch the *challenger* model. This is the best model in *development*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "challenger_model_df = project.fetch_runs_table(\n",
    "    query='(`sys/group_tags`:stringSet CONTAINS \"development\") AND (`sys/tags`:stringSet CONTAINS \"trial-level\")',\n",
    "    columns=[score_namespace],\n",
    "    sort_by=score_namespace,\n",
    "    ascending=True,\n",
    "    limit=1,\n",
    ").to_pandas()\n",
    "\n",
    "challenger_model_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get scores and IDs of challenger and champion models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    champion_model_id = champion_model_df[\"sys/id\"].values[0]\n",
    "    champion_model_score = champion_model_df[score_namespace].values[0]\n",
    "    print(f\"Champion model ID: {champion_model_id} and score: {champion_model_score}\")\n",
    "    NO_CHAMPION = False\n",
    "except KeyError:\n",
    "    print(\"‚ùå No model found in production\")\n",
    "    NO_CHAMPION = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "challenger_model_id = challenger_model_df[\"sys/id\"].values[0]\n",
    "challenger_model_score = challenger_model_df[score_namespace].values[0]\n",
    "print(f\"Challenger model ID: {challenger_model_id} and score: {challenger_model_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Promote challenger to champion if score is better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if NO_CHAMPION:\n",
    "    print(f\"Promoting {challenger_model_id} to Production\")\n",
    "    with neptune.init_run(with_id=challenger_model_id) as challenger_model:\n",
    "        challenger_model[\"sys/group_tags\"].add(\"production\")\n",
    "        challenger_model[\"sys/group_tags\"].remove(\"development\")\n",
    "\n",
    "elif challenger_model_score < champion_model_score:\n",
    "    print(\"Challenger is better than champion\")\n",
    "\n",
    "    print(f\"Archiving champion model {champion_model_id}\")\n",
    "    with neptune.init_run(with_id=champion_model_id) as champion_model:\n",
    "        champion_model[\"sys/group_tags\"].remove(\"production\")\n",
    "        champion_model[\"sys/group_tags\"].add(\"archived\")\n",
    "\n",
    "    print(f\"Promoting {challenger_model_id} to Production\")\n",
    "    with neptune.init_run(with_id=challenger_model_id) as challenger_model:\n",
    "        challenger_model[\"sys/group_tags\"].add(\"production\")\n",
    "        challenger_model[\"sys/group_tags\"].remove(\"development\")\n",
    "\n",
    "else:\n",
    "    print(\"Champion model is better than challenger\")\n",
    "    print(f\"Archiving challenger model {challenger_model_id}\")\n",
    "    with neptune.init_run(with_id=challenger_model_id) as challenger_model:\n",
    "        challenger_model[\"sys/group_tags\"].add(\"archived\")\n",
    "        challenger_model[\"sys/group_tags\"].remove(\"development\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitor model in production\n",
    "In this section, we'll:\n",
    "1. Download the model binary from the run to make predictions in production.\n",
    "2. Use [EvidentlyAI](https://www.evidentlyai.com/) to monitor the model in production.\n",
    "\n",
    "We'll use a modified version of a tutorial from the [Evidently documentation](https://docs.evidentlyai.com/get-started/tutorial)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup\n",
    "\n",
    "We'll use an example dataset and mock historical predictions to use as a reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "from evidently.report import Report\n",
    "from evidently.metric_preset import RegressionPreset\n",
    "from evidently.metrics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = fetch_california_housing(as_frame=True)\n",
    "housing_data = data.frame\n",
    "\n",
    "housing_data.rename(columns={\"MedHouseVal\": \"target\"}, inplace=True)\n",
    "\n",
    "reference = housing_data.sample(n=10000, replace=False)\n",
    "reference[\"prediction\"] = reference[\"target\"].values + np.random.normal(\n",
    "    0, 0.1, reference.shape[0]\n",
    ")  # Mocking historical predictions\n",
    "\n",
    "current = housing_data.sample(n=10000, replace=False)\n",
    "dcurrent = xgb.DMatrix(current.drop(\"target\", axis=1), label=current[\"target\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download saved model from Neptune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "production_model_id = (\n",
    "    project.fetch_runs_table(\n",
    "        query='`sys/group_tags`:stringSet CONTAINS \"production\"',\n",
    "        columns=[],\n",
    "        sort_by=score_namespace,\n",
    "        ascending=True,\n",
    "        limit=1,\n",
    "    )\n",
    "    .to_pandas()[\"sys/id\"]\n",
    "    .values[0]\n",
    ")\n",
    "\n",
    "production_model = neptune.init_run(with_id=production_model_id)\n",
    "production_model[\"training/pickled_model\"].download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make predictions from downloaded model on current test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "\n",
    "with open(\"pickled_model.pkl\", \"rb\") as f:\n",
    "    model = pkl.load(f)\n",
    "\n",
    "current[\"prediction\"] = model.predict(dcurrent)\n",
    "\n",
    "current"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate regression report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_performance_report = Report(metrics=[RegressionPreset()])\n",
    "\n",
    "reg_performance_report.run(reference_data=reference, current_data=current)\n",
    "\n",
    "reg_performance_report.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload report to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_performance_report.save_html(\"report.html\")\n",
    "production_model[\"production/report\"].upload(\"report.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload metrics to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neptune.utils import stringify_unsupported\n",
    "\n",
    "production_model[\"production/metrics\"] = stringify_unsupported(\n",
    "    reg_performance_report.as_dict()[\"metrics\"][0]\n",
    ")\n",
    "production_model.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see an example of a production monitoring custom dashboard [here](https://app.neptune.ai/o/showcase/org/e2e-tracking/runs/details?viewId=9cb5bc7c-3bce-4c69-8f5c-90d3d9cc682c&detailsTab=dashboard&dashboardId=9cb5c04f-e405-497f-a028-d80d0a82c55a&shortId=EET-36&type=run&compare=EwTgNAjJ1cP3KTJA&lbViewUnpacked=true&sortBy=%5B%22training%2Fearly_stopping%2Fbest_score%22%5D&sortFieldType=%5B%22string%22%5D&sortFieldAggregationMode=%5B%22auto%22%5D&sortDirection=%5B%22ascending%22%5D&groupBy=%5B%22sys%2Fgroup_tags%22%5D&groupByFieldType=%5B%22stringSet%22%5D&groupByFieldAggregationMode=%5B%22auto%22%5D&suggestionsEnabled=false&query=((%60sys%2Ftags%60%3AstringSet%20CONTAINS%20%22trial-level%22)))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These metrics can then be fetched downstream to trigger a model refresh or retraining, if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retraining_threshold = 0.5  # example threshold\n",
    "\n",
    "if production_model[\"production/metrics/result/current/rmse\"].fetch() > retraining_threshold:\n",
    "    print(\"Model degradation detected. Retraining model...\")\n",
    "    ...\n",
    "else:\n",
    "    print(\"Model performance within expectations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Optional) Maintain a history of production metrics\n",
    "\n",
    "The above method only logs the latest production metrics by overwriting the previous report and metrics.  \n",
    "To maintain a history of production metrics:\n",
    "\n",
    "* Log each report under a different folder, as shown below:\n",
    "\n",
    "  ```py\n",
    "  from datetime import datetime\n",
    "  npt_model[f\"production/{datetime.now().date()}/report\"].upload(\"report.html\")\n",
    "  ```\n",
    "  This will create a new folder for each day and upload the report to that folder.\n",
    "\n",
    "* [Log metrics as a series](https://docs-legacy.neptune.ai/logging/series/#numerical-series-floatseries), as shown below:\n",
    "\n",
    "  ```py\n",
    "  npt_model[\"production/rmse\"].append(rmse)\n",
    "  ```\n",
    "  This will let you visualize the production metrics over time as a chart."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop Neptune objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project.stop()\n",
    "production_model.stop()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Neptune_E2E_Model_Tracking.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "vscode": {
   "interpreter": {
    "hash": "a9715cf0b0024f6e1c62cb31a4f1f43970eb41991212681878768b4bfe53050a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
