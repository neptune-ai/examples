{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f56b4f38",
   "metadata": {},
   "source": [
    "# Using Neptune for logging PyTorch training jobs on SageMaker\n",
    "\n",
    "<div class=\"alert alert-info\">This notebook should be run from a SageMaker notebook.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f069b582-bada-4274-b20c-c710e9f17bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q -U sagemaker neptune-client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d56133b-5e4b-45dd-b4f3-8938821116c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python Built-Ins:\n",
    "from datetime import datetime\n",
    "import os\n",
    "import json\n",
    "import logging\n",
    "from tempfile import TemporaryFile\n",
    "import time\n",
    "\n",
    "# External Dependencies:\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "import numpy as np\n",
    "import sagemaker\n",
    "from sagemaker.pytorch.estimator import PyTorch\n",
    "\n",
    "# Neptune\n",
    "import neptune.new as neptune\n",
    "\n",
    "# Initialize the session\n",
    "sess = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "# Configuration:\n",
    "bucket_name = sess.default_bucket()\n",
    "prefix = \"mnist/\"\n",
    "output_path = f\"s3://{bucket_name}/{prefix[:-1]}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d1b8ac",
   "metadata": {},
   "source": [
    "## The example use case: MNIST\n",
    "\n",
    "MNIST is a widely used dataset for handwritten digit classification. It consists of 70,000 labeled 28x28 pixel grayscale images of handwritten digits. The dataset is split into 60,000 training images and 10,000 test images.\n",
    "\n",
    "In this example, we download the MNIST data from a public S3 bucket and upload it to your default SageMaker bucket as selected above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94bafde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_sample_data(\n",
    "    to_bucket: str,\n",
    "    to_prefix: str,\n",
    "    from_bucket: str = \"sagemaker-sample-files\",\n",
    "    from_prefix: str = \"datasets/image/MNIST\",\n",
    "    dataset: str = \"mnist-train\",\n",
    "):\n",
    "    DATASETS = {\n",
    "        \"mnist-train\": [\"train-images-idx3-ubyte.gz\", \"train-labels-idx1-ubyte.gz\"],\n",
    "        \"mnist-test\": [\"t10k-images-idx3-ubyte.gz\", \"t10k-labels-idx1-ubyte.gz\"],\n",
    "    }\n",
    "\n",
    "    if dataset not in DATASETS:\n",
    "        raise ValueError(f\"dataset '{dataset}' not in known set: {set(DATASETS.keys())}\")\n",
    "\n",
    "    if len(from_prefix) and not from_prefix.endswith(\"/\"):\n",
    "        from_prefix += \"/\"\n",
    "    if len(to_prefix) and not to_prefix.endswith(\"/\"):\n",
    "        to_prefix += \"/\"\n",
    "\n",
    "    s3client = boto3.client(\"s3\")\n",
    "    for key in DATASETS[dataset]:\n",
    "        # If you're in the same region as the source bucket, you might consider copy_object() instead:\n",
    "        with TemporaryFile() as ftmp:\n",
    "            s3client.download_fileobj(from_bucket, f\"{from_prefix}{key}\", ftmp)\n",
    "            ftmp.seek(0)\n",
    "            s3client.upload_fileobj(ftmp, to_bucket, f\"{to_prefix}{key}\")\n",
    "\n",
    "\n",
    "train_prefix = f\"{prefix}data/train\"\n",
    "fetch_sample_data(to_bucket=bucket_name, to_prefix=train_prefix, dataset=\"mnist-train\")\n",
    "train_s3uri = f\"s3://{bucket_name}/{train_prefix}\"\n",
    "print(f\"Uploaded training data to {train_s3uri}\")\n",
    "\n",
    "test_prefix = f\"{prefix}data/test\"\n",
    "fetch_sample_data(to_bucket=bucket_name, to_prefix=test_prefix, dataset=\"mnist-test\")\n",
    "test_s3uri = f\"s3://{bucket_name}/{test_prefix}\"\n",
    "print(f\"Uploaded training data to {test_s3uri}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd2050d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training data:\")\n",
    "!aws s3 ls --recursive $train_s3uri\n",
    "print(\"Test data:\")\n",
    "!aws s3 ls --recursive $test_s3uri"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6c1caf03",
   "metadata": {},
   "source": [
    "## Train \n",
    "\n",
    "We are going to use the [SageMaker PyTorch Estimator](https://sagemaker.readthedocs.io/en/stable/frameworks/pytorch/sagemaker.pytorch.html).\n",
    "The estimator uses the code from the `code/` directory. The code is adapted from the [Amazon SageMaker Examples repository](https://github.com/aws/amazon-sagemaker-examples/tree/main/advanced_functionality/multi_model_pytorch).\n",
    "What was changed in the code is that Neptune logging was added to the `code/train.py` script.\n",
    "\n",
    "```diff\n",
    "[...]\n",
    "\n",
    "def train(args):\n",
    "+   run = neptune.init_run(tags=[\"sagemaker\"])\n",
    "    \n",
    "    [...]\n",
    "    \n",
    "+   run[\"training/args\"] = args\n",
    "+   run[\"training/model/loss_fn\"] = type(loss_fn).__name__\n",
    "+   run[\"training/model/model\"] = type(net).__name__\n",
    "+   run[\"training/model/optimizer\"] = type(optimizer).__name__\n",
    "\n",
    "    logger.info(\"Start training ...\")\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        net.train()\n",
    "        for batch_idx, (imgs, labels) in enumerate(train_loader, 1):\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "            output = net(imgs)\n",
    "            loss = loss_fn(output, labels)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            [...]\n",
    "            \n",
    "+           run[\"training/train/batch/loss\"].log(loss.item())\n",
    "\n",
    "        # test the model\n",
    "+       train_loss, train_acc = test(net, train_loader, device)\n",
    "+       run[\"training/train/epoch/loss\"].log(train_loss)\n",
    "+       run[\"training/train/epoch/accuracy\"].log(train_acc)\n",
    "        \n",
    "+       test_loss, test_acc = test(net, test_loader, device)\n",
    "+       run[\"training/test/epoch/loss\"].log(test_loss)\n",
    "+       run[\"training/test/epoch/accuracy\"].log(test_acc)\n",
    "\n",
    "[...]\n",
    "        \n",
    "```\n",
    "\n",
    "Another difference is that we need `neptune-client` as an additional dependency, so we add it to `code/requirements.txt`. The SageMaker Estimator by default installs all the dependencies defined there.\n",
    "\n",
    "Because SageMaker does not use python>=3.6 by default, we need to use a custom Docker image to train the model as Python 3.6 has already reached end-of-life and is not supported by Neptune. For this purpose, we use [one of the images provided by AWS](https://github.com/aws/deep-learning-containers/blob/master/available_images.md) as `image_uri`. \n",
    " \n",
    "Moreover, we provide the Neptune API token and project name as environment variables to the training job via the `environment` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040cceb9-aacf-4cd1-8a0b-3c99fb724db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    \"batch-size\": 128,\n",
    "    \"epochs\": 5,\n",
    "    \"learning-rate\": 1e-3,\n",
    "    \"log-interval\": 100,\n",
    "}\n",
    "\n",
    "image_uri = \"763104351884.dkr.ecr.us-east-1.amazonaws.com/pytorch-training:1.12.1-cpu-py38-ubuntu20.04-sagemaker\"\n",
    "\n",
    "estimator = PyTorch(\n",
    "    entry_point=\"train.py\",\n",
    "    source_dir=\"code\",  # directory of your training script\n",
    "    role=role,\n",
    "    image_uri=image_uri,\n",
    "    instance_type=\"ml.m5.large\",\n",
    "    instance_count=1,\n",
    "    output_path=output_path,\n",
    "    hyperparameters=hyperparameters,\n",
    "    environment={\n",
    "        \"NEPTUNE_API_TOKEN\": neptune.ANONYMOUS_API_TOKEN,\n",
    "        \"NEPTUNE_PROJECT\": \"common/showroom\"\n",
    "    }\n",
    ")\n",
    "\n",
    "estimator.fit({\"training\": train_s3uri, \"testing\": test_s3uri})"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
