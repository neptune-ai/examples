{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text classification using Keras with Neptune tracking\n",
    "Notebook inspired from https://keras.io/examples/nlp/text_classification_from_scratch/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Neptune) Install the neptune-notebooks widget (optional)\n",
    "The neptune-notebooks jupyter extension lets you version, manage and share notebook checkpoints in your projects, without leaving your notebook.  \n",
    "[Read the docs](https://docs.neptune.ai/integrations-and-supported-tools/ide-and-notebooks/jupyter-lab-and-jupyter-notebook)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: neptune-tensorflow-keras in c:\\users\\siddh\\miniconda3\\envs\\py38\\lib\\site-packages (2.0.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\siddh\\miniconda3\\envs\\py38\\lib\\site-packages (1.24.2)\n",
      "Requirement already satisfied: pydot in c:\\users\\siddh\\miniconda3\\envs\\py38\\lib\\site-packages (1.4.2)\n",
      "Requirement already satisfied: tensorflow in c:\\users\\siddh\\miniconda3\\envs\\py38\\lib\\site-packages (2.11.0)\n",
      "Requirement already satisfied: neptune>=1.0.0 in c:\\users\\siddh\\miniconda3\\envs\\py38\\lib\\site-packages (from neptune-tensorflow-keras) (1.0.2)\n",
      "Requirement already satisfied: pyparsing>=2.1.4 in c:\\users\\siddh\\miniconda3\\envs\\py38\\lib\\site-packages (from pydot) (3.0.9)\n",
      "Requirement already satisfied: tensorflow-intel==2.11.0 in c:\\users\\siddh\\miniconda3\\envs\\py38\\lib\\site-packages (from tensorflow) (2.11.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\siddh\\miniconda3\\envs\\py38\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (22.0)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in c:\\users\\siddh\\miniconda3\\envs\\py38\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (3.19.6)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\siddh\\miniconda3\\envs\\py38\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: keras<2.12,>=2.11.0 in c:\\users\\siddh\\miniconda3\\envs\\py38\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (2.11.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\siddh\\miniconda3\\envs\\py38\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (0.29.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\siddh\\miniconda3\\envs\\py38\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\siddh\\miniconda3\\envs\\py38\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: tensorboard<2.12,>=2.11 in c:\\users\\siddh\\miniconda3\\envs\\py38\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (2.11.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\siddh\\miniconda3\\envs\\py38\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (65.5.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\siddh\\miniconda3\\envs\\py38\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.51.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\siddh\\miniconda3\\envs\\py38\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (4.4.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\siddh\\miniconda3\\envs\\py38\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\siddh\\miniconda3\\envs\\py38\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in c:\\users\\siddh\\miniconda3\\envs\\py38\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (0.4.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\siddh\\miniconda3\\envs\\py38\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.3.0)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in c:\\users\\siddh\\miniconda3\\envs\\py38\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (22.12.6)\n",
      "Requirement already satisfied: tensorflow-estimator<2.12,>=2.11.0 in c:\\users\\siddh\\miniconda3\\envs\\py38\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (2.11.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\siddh\\miniconda3\\envs\\py38\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (3.7.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\siddh\\miniconda3\\envs\\py38\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (14.0.6)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\siddh\\miniconda3\\envs\\py38\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.14.1)\n",
      "Requirement already satisfied: urllib3 in c:\\users\\siddh\\miniconda3\\envs\\py38\\lib\\site-packages (from neptune>=1.0.0->neptune-tensorflow-keras) (1.26.13)\n",
      "Requirement already satisfied: websocket-client!=1.0.0,>=0.35.0 in c:\\users\\siddh\\miniconda3\\envs\\py38\\lib\\site-packages (from neptune>=1.0.0->neptune-tensorflow-keras) (1.4.2)\n",
      "Requirement already satisfied: future>=0.17.1 in c:\\users\\siddh\\miniconda3\\envs\\py38\\lib\\site-packages (from neptune>=1.0.0->neptune-tensorflow-keras) (0.18.2)\n",
      "Requirement already satisfied: bravado<12.0.0,>=11.0.0 in c:\\users\\siddh\\miniconda3\\envs\\py38\\lib\\site-packages (from neptune>=1.0.0->neptune-tensorflow-keras) (11.0.3)\n",
      "Requirement already satisfied: GitPython>=2.0.8 in c:\\users\\siddh\\miniconda3\\envs\\py38\\lib\\site-packages (from neptune>=1.0.0->neptune-tensorflow-keras) (3.1.29)\n",
      "Requirement already satisfied: pandas in c:\\users\\siddh\\miniconda3\\envs\\py38\\lib\\site-packages (from neptune>=1.0.0->neptune-tensorflow-keras) (1.5.3)\n",
      "Requirement already satisfied: swagger-spec-validator>=2.7.4 in c:\\users\\siddh\\miniconda3\\envs\\py38\\lib\\site-packages (from neptune>=1.0.0->neptune-tensorflow-keras) (3.0.3)\n",
      "Requirement already satisfied: psutil in c:\\users\\siddh\\miniconda3\\envs\\py38\\lib\\site-packages (from neptune>=1.0.0->neptune-tensorflow-keras) (5.9.4)\n",
      "Requirement already satisfied: oauthlib>=2.1.0 in c:\\users\\siddh\\miniconda3\\envs\\py38\\lib\\site-packages (from neptune>=1.0.0->neptune-tensorflow-keras) (3.2.2)\n",
      "Requirement already satisfied: PyJWT in c:\\users\\siddh\\miniconda3\\envs\\py38\\lib\\site-packages (from neptune>=1.0.0->neptune-tensorflow-keras) (2.6.0)\n",
      "Requirement already satisfied: requests-oauthlib>=1.0.0 in c:\\users\\siddh\\miniconda3\\envs\\py38\\lib\\site-packages (from neptune>=1.0.0->neptune-tensorflow-keras) (1.3.1)\n",
      "Requirement already satisfied: boto3>=1.16.0 in c:\\users\\siddh\\miniconda3\\envs\\py38\\lib\\site-packages (from neptune>=1.0.0->neptune-tensorflow-keras) (1.24.59)\n",
      "Requirement already satisfied: Pillow>=1.1.6 in c:\\users\\siddh\\miniconda3\\envs\\py38\\lib\\site-packages (from neptune>=1.0.0->neptune-tensorflow-keras) (9.4.0)\n",
      "Requirement already satisfied: click>=7.0 in c:\\users\\siddh\\miniconda3\\envs\\py38\\lib\\site-packages (from neptune>=1.0.0->neptune-tensorflow-keras) (8.1.3)\n",
      "Requirement already satisfied: requests>=2.20.0 in c:\\users\\siddh\\miniconda3\\envs\\py38\\lib\\site-packages (from neptune>=1.0.0->neptune-tensorflow-keras) (2.28.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\siddh\\miniconda3\\envs\\py38\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.11.0->tensorflow) (0.38.4)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in c:\\users\\siddh\\miniconda3\\envs\\py38\\lib\\site-packages (from boto3>=1.16.0->neptune>=1.0.0->neptune-tensorflow-keras) (0.10.0)\n",
      "Requirement already satisfied: botocore<1.28.0,>=1.27.59 in c:\\users\\siddh\\miniconda3\\envs\\py38\\lib\\site-packages (from boto3>=1.16.0->neptune>=1.0.0->neptune-tensorflow-keras) (1.27.59)\n",
      "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in c:\\users\\siddh\\miniconda3\\envs\\py38\\lib\\site-packages (from boto3>=1.16.0->neptune>=1.0.0->neptune-tensorflow-keras) (0.6.0)\n",
      "Requirement already satisfied: simplejson in c:\\users\\siddh\\miniconda3\\envs\\py38\\lib\\site-packages (from bravado<12.0.0,>=11.0.0->neptune>=1.0.0->neptune-tensorflow-keras) (3.18.0)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\siddh\\miniconda3\\envs\\py38\\lib\\site-packages (from bravado<12.0.0,>=11.0.0->neptune>=1.0.0->neptune-tensorflow-keras) (6.0)\n",
      "Requirement already satisfied: monotonic in c:\\users\\siddh\\miniconda3\\envs\\py38\\lib\\site-packages (from bravado<12.0.0,>=11.0.0->neptune>=1.0.0->neptune-tensorflow-keras) (1.6)\n",
      "Requirement already satisfied: python-dateutil in c:\\users\\siddh\\miniconda3\\envs\\py38\\lib\\site-packages (from bravado<12.0.0,>=11.0.0->neptune>=1.0.0->neptune-tensorflow-keras) (2.8.2)\n",
      "Requirement already satisfied: msgpack in c:\\users\\siddh\\miniconda3\\envs\\py38\\lib\\site-packages (from bravado<12.0.0,>=11.0.0->neptune>=1.0.0->neptune-tensorflow-keras) (1.0.4)\n",
      "Requirement already satisfied: bravado-core>=5.16.1 in c:\\users\\siddh\\miniconda3\\envs\\py38\\lib\\site-packages (from bravado<12.0.0,>=11.0.0->neptune>=1.0.0->neptune-tensorflow-keras) (5.17.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\siddh\\miniconda3\\envs\\py38\\lib\\site-packages (from click>=7.0->neptune>=1.0.0->neptune-tensorflow-keras) (0.4.6)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in c:\\users\\siddh\\miniconda3\\envs\\py38\\lib\\site-packages (from GitPython>=2.0.8->neptune>=1.0.0->neptune-tensorflow-keras) (4.0.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\siddh\\miniconda3\\envs\\py38\\lib\\site-packages (from requests>=2.20.0->neptune>=1.0.0->neptune-tensorflow-keras) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\siddh\\miniconda3\\envs\\py38\\lib\\site-packages (from requests>=2.20.0->neptune>=1.0.0->neptune-tensorflow-keras) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\siddh\\miniconda3\\envs\\py38\\lib\\site-packages (from requests>=2.20.0->neptune>=1.0.0->neptune-tensorflow-keras) (3.4)\n",
      "Requirement already satisfied: jsonschema in c:\\users\\siddh\\miniconda3\\envs\\py38\\lib\\site-packages (from swagger-spec-validator>=2.7.4->neptune>=1.0.0->neptune-tensorflow-keras) (4.17.3)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\siddh\\miniconda3\\envs\\py38\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (1.8.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\siddh\\miniconda3\\envs\\py38\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2.2.2)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\siddh\\miniconda3\\envs\\py38\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2.15.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\siddh\\miniconda3\\envs\\py38\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (0.6.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\siddh\\miniconda3\\envs\\py38\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (3.3.7)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\siddh\\miniconda3\\envs\\py38\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (0.4.6)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\siddh\\miniconda3\\envs\\py38\\lib\\site-packages (from pandas->neptune>=1.0.0->neptune-tensorflow-keras) (2022.7)\n",
      "Requirement already satisfied: jsonref in c:\\users\\siddh\\miniconda3\\envs\\py38\\lib\\site-packages (from bravado-core>=5.16.1->bravado<12.0.0,>=11.0.0->neptune>=1.0.0->neptune-tensorflow-keras) (1.0.1)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in c:\\users\\siddh\\miniconda3\\envs\\py38\\lib\\site-packages (from gitdb<5,>=4.0.1->GitPython>=2.0.8->neptune>=1.0.0->neptune-tensorflow-keras) (5.0.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\siddh\\miniconda3\\envs\\py38\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (5.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\siddh\\miniconda3\\envs\\py38\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (4.9)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\siddh\\miniconda3\\envs\\py38\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in c:\\users\\siddh\\miniconda3\\envs\\py38\\lib\\site-packages (from markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (4.13.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\siddh\\miniconda3\\envs\\py38\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: importlib-resources>=1.4.0 in c:\\users\\siddh\\miniconda3\\envs\\py38\\lib\\site-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune>=1.0.0->neptune-tensorflow-keras) (5.10.1)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in c:\\users\\siddh\\miniconda3\\envs\\py38\\lib\\site-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune>=1.0.0->neptune-tensorflow-keras) (0.19.2)\n",
      "Requirement already satisfied: attrs>=17.4.0 in c:\\users\\siddh\\miniconda3\\envs\\py38\\lib\\site-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune>=1.0.0->neptune-tensorflow-keras) (22.1.0)\n",
      "Requirement already satisfied: pkgutil-resolve-name>=1.3.10 in c:\\users\\siddh\\miniconda3\\envs\\py38\\lib\\site-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune>=1.0.0->neptune-tensorflow-keras) (1.3.10)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\siddh\\miniconda3\\envs\\py38\\lib\\site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (3.11.0)\n",
      "Requirement already satisfied: rfc3987 in c:\\users\\siddh\\miniconda3\\envs\\py38\\lib\\site-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune>=1.0.0->neptune-tensorflow-keras) (1.3.8)\n",
      "Requirement already satisfied: uri-template in c:\\users\\siddh\\miniconda3\\envs\\py38\\lib\\site-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune>=1.0.0->neptune-tensorflow-keras) (1.2.0)\n",
      "Requirement already satisfied: webcolors>=1.11 in c:\\users\\siddh\\miniconda3\\envs\\py38\\lib\\site-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune>=1.0.0->neptune-tensorflow-keras) (1.12)\n",
      "Requirement already satisfied: rfc3339-validator in c:\\users\\siddh\\miniconda3\\envs\\py38\\lib\\site-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune>=1.0.0->neptune-tensorflow-keras) (0.1.4)\n",
      "Requirement already satisfied: isoduration in c:\\users\\siddh\\miniconda3\\envs\\py38\\lib\\site-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune>=1.0.0->neptune-tensorflow-keras) (20.11.0)\n",
      "Requirement already satisfied: jsonpointer>1.13 in c:\\users\\siddh\\miniconda3\\envs\\py38\\lib\\site-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune>=1.0.0->neptune-tensorflow-keras) (2.3)\n",
      "Requirement already satisfied: fqdn in c:\\users\\siddh\\miniconda3\\envs\\py38\\lib\\site-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune>=1.0.0->neptune-tensorflow-keras) (1.5.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\siddh\\miniconda3\\envs\\py38\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: arrow>=0.15.0 in c:\\users\\siddh\\miniconda3\\envs\\py38\\lib\\site-packages (from isoduration->jsonschema->swagger-spec-validator>=2.7.4->neptune>=1.0.0->neptune-tensorflow-keras) (1.2.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -U neptune-tensorflow-keras numpy pydot tensorflow graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_files(source: str, destination: str) -> None:\n",
    "    \"\"\"Extracts files from the source archive to the destination path\n",
    "\n",
    "    Args:\n",
    "        source (str): Archive file path\n",
    "        destination (str): Extract destination path\n",
    "    \"\"\"\n",
    "\n",
    "    import tarfile\n",
    "\n",
    "    print(\"Extracting data...\")\n",
    "    with tarfile.open(source) as f:\n",
    "        f.extractall(destination)\n",
    "\n",
    "\n",
    "def prep_data(imdb_folder: str, dest_path: str) -> None:\n",
    "    \"\"\"Removes unnecessary folders/files and renames source folder\n",
    "\n",
    "    Args:\n",
    "        imdb_folder (str): Path of the aclImdb folder\n",
    "        dest_name (str): Destination folder to which the aclImdb folder has to be renamed to\n",
    "    \"\"\"\n",
    "    import os\n",
    "    import shutil\n",
    "\n",
    "    shutil.rmtree(f\"{imdb_folder}/train/unsup\")\n",
    "    os.remove(f\"{imdb_folder.rsplit('/', maxsplit=1)[0]}/aclImdb_v1.tar.gz\")\n",
    "\n",
    "    if os.path.exists(dest_path):\n",
    "        shutil.rmtree(dest_path)\n",
    "\n",
    "    os.rename(imdb_folder, dest_path)\n",
    "    print(f\"{imdb_folder} renamed to {dest_path}\")\n",
    "\n",
    "\n",
    "def custom_standardization(input_data):\n",
    "    import string\n",
    "    import re\n",
    "\n",
    "    lowercase = tf.strings.lower(input_data)\n",
    "    stripped_html = tf.strings.regex_replace(lowercase, \"<br />\", \" \")\n",
    "    return tf.strings.regex_replace(stripped_html, f\"[{re.escape(string.punctuation)}]\", \"\")\n",
    "\n",
    "\n",
    "def vectorize_text(text, label):\n",
    "    text = tf.expand_dims(text, -1)\n",
    "    return vectorize_layer(text), label\n",
    "\n",
    "\n",
    "def build_model(model_params: dict, data_params: dict):\n",
    "    \"\"\"Accepts model and data parameters to build and compile a keras model\n",
    "\n",
    "    Args:\n",
    "        model_params (dict): Model parameters\n",
    "        data_params (dict): Data parameters\n",
    "\n",
    "    Returns:\n",
    "        A compiled keras model\n",
    "    \"\"\"\n",
    "\n",
    "    import tensorflow as tf\n",
    "    from tensorflow.keras import layers\n",
    "\n",
    "    # A integer input for vocab indices.\n",
    "    inputs = tf.keras.Input(shape=(None,), dtype=\"int64\")\n",
    "\n",
    "    # Next, we add a layer to map those vocab indices into a space of dimensionality\n",
    "    # 'embedding_dim'.\n",
    "    x = layers.Embedding(data_params[\"max_features\"], data_params[\"embedding_dim\"])(inputs)\n",
    "    x = layers.Dropout(model_params[\"dropout\"])(x)\n",
    "\n",
    "    # Conv1D + global max pooling\n",
    "    x = layers.Conv1D(\n",
    "        data_params[\"embedding_dim\"],\n",
    "        model_params[\"kernel_size\"],\n",
    "        padding=\"valid\",\n",
    "        activation=model_params[\"activation\"],\n",
    "        strides=model_params[\"strides\"],\n",
    "    )(x)\n",
    "    x = layers.Conv1D(\n",
    "        data_params[\"embedding_dim\"],\n",
    "        model_params[\"kernel_size\"],\n",
    "        padding=\"valid\",\n",
    "        activation=model_params[\"activation\"],\n",
    "        strides=model_params[\"strides\"],\n",
    "    )(x)\n",
    "    x = layers.GlobalMaxPooling1D()(x)\n",
    "\n",
    "    # We add a vanilla hidden layer:\n",
    "    x = layers.Dense(data_params[\"embedding_dim\"], activation=model_params[\"activation\"])(x)\n",
    "    x = layers.Dropout(model_params[\"dropout\"])(x)\n",
    "\n",
    "    # We project onto a single unit output layer, and squash it with a sigmoid:\n",
    "    predictions = layers.Dense(1, activation=\"sigmoid\", name=\"predictions\")(x)\n",
    "\n",
    "    keras_model = tf.keras.Model(inputs, predictions)\n",
    "\n",
    "    # Compile the model with binary crossentropy loss and an adam optimizer.\n",
    "    keras_model.compile(\n",
    "        loss=model_params[\"loss\"],\n",
    "        optimizer=model_params[\"optimizer\"],\n",
    "        metrics=model_params[\"metrics\"],\n",
    "    )\n",
    "\n",
    "    return keras_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Neptune) Import Neptune and initialize a project\n",
    "**A project is a collection of runs, models, and other metadata created by project members.** Typically you should create one project per machine learning task, to make it easy to compare runs that are connected to building certain kinds of ML model.  \n",
    "[Read the docs](https://docs.neptune.ai/you-should-know/core-concepts#project)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"NEPTUNE_PROJECT\"] = \"common/project-text-classification\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need a Neptune API token to be able to log to Neptune.\n",
    "Read how to get and use one [here](https://docs.neptune.ai/setup/setting_api_token/#setting-your-api-token).\n",
    "\n",
    "**or** \n",
    "\n",
    "If you don't have an API token, you can use the `neptune.ANONYMOUS_API_TOKEN` to log to a public project.  \n",
    "To log anonymously to a public project, set it as your environment variable as below:\n",
    "\n",
    "```python\n",
    "os.environ[\"NEPTUNE_API_TOKEN\"] = neptune.ANONYMOUS_API_TOKEN\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://app.neptune.ai/showcase/project-text-classification/\n"
     ]
    }
   ],
   "source": [
    "import neptune\n",
    "\n",
    "project = neptune.init_project()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation\n",
    "We are using the IMDB sentiment analysis data available at https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz. For the purposes of this demo, we've uploaded this data to S3 at https://neptune-examples.s3.us-east-2.amazonaws.com/data/text-classification/aclImdb_v1.tar.gz and will be downloading it from there."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Neptune) Track datasets using Neptune\n",
    "Neptune lets you track pointers to datasets, models, and other artifacts stored locally or in S3.  \n",
    "To use this, you will need to have your `AWS_ACCESS_KEY_ID` and `AWS_SECRET_ACCESS_KEY` environment variables set.  \n",
    "[Read the docs](https://docs.neptune.ai/how-to-guides/data-versioning)\n",
    "\n",
    "Since this dataset will be used among all the runs in the project, we track it at the project level.\n",
    "Read more about logging project-level metadata [here](https://docs.neptune.ai/logging/project_metadata/#logging-project-level-metadata).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project[\"keras/data/files\"].track_files(\n",
    "    \"s3://neptune-examples/data/text-classification/aclImdb_v1.tar.gz\"\n",
    ")\n",
    "project.sync()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Neptune) Download files from S3 using Neptune\n",
    "You can also download tracked files from S3 using Neptune, without having to write boilerplate boto3 code.\n",
    "Read the artifact API reference to know more: https://docs.neptune.ai/api/field_types/#download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data...\n"
     ]
    }
   ],
   "source": [
    "print(\"Downloading data...\")\n",
    "project[\"keras/data/files\"].download(\"..\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data...\n"
     ]
    },
    {
     "ename": "PermissionError",
     "evalue": "[WinError 5] Access is denied: '../aclImdb' -> '../data'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m extract_files(source\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../aclImdb_v1.tar.gz\u001b[39m\u001b[38;5;124m\"\u001b[39m, destination\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m..\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m \u001b[43mprep_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimdb_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../aclImdb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdest_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../data\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[8], line 32\u001b[0m, in \u001b[0;36mprep_data\u001b[1;34m(imdb_folder, dest_path)\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(dest_path):\n\u001b[0;32m     30\u001b[0m     shutil\u001b[38;5;241m.\u001b[39mrmtree(dest_path)\n\u001b[1;32m---> 32\u001b[0m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrename\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimdb_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdest_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimdb_folder\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m renamed to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdest_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mPermissionError\u001b[0m: [WinError 5] Access is denied: '../aclImdb' -> '../data'"
     ]
    }
   ],
   "source": [
    "extract_files(source=\"../aclImdb_v1.tar.gz\", destination=\"..\")\n",
    "prep_data(\n",
    "    imdb_folder=\"../aclImdb\", dest_path=\"../data\"\n",
    ")  # If you get a permission error here, you can manually rename the `aclImdb` folder to `data`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (Neptune) Upload dataset sample to Neptune project\n",
    "In addition to tracking external files, you can also upload them directly to Neptune.\n",
    "Such uploaded files can be visualized directly in the Neptune app.  \n",
    "[Read more here](https://docs.neptune.ai/logging/files/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "base_namespace = \"keras/data/sample/\"\n",
    "\n",
    "project[base_namespace][\"train/pos\"].upload(\n",
    "    f\"../data/train/pos/{random.choice(os.listdir('../data/train/pos'))}\"\n",
    ")\n",
    "project[base_namespace][\"train/neg\"].upload(\n",
    "    f\"../data/train/neg/{random.choice(os.listdir('../data/train/neg'))}\"\n",
    ")\n",
    "project[base_namespace][\"test/pos\"].upload(\n",
    "    f\"../data/test/pos/{random.choice(os.listdir('../data/test/pos'))}\"\n",
    ")\n",
    "project[base_namespace][\"test/neg\"].upload(\n",
    "    f\"../data/test/neg/{random.choice(os.listdir('../data/test/neg'))}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Neptune) Initialize a run\n",
    "**A run is a namespace inside a project where you log metadata.** Typically, you create a run every time you execute a script that does model training, re-training, or inference.  \n",
    "[Read the docs](https://docs.neptune.ai/you-should-know/core-concepts#run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\miniconda3\\envs\\py38\\lib\\site-packages\\neptune\\common\\warnings.py:62: NeptuneWarning: To avoid unintended consumption of logging hours during interactive sessions, the following monitoring options are disabled unless set to 'True' when initializing the run: 'capture_stdout', 'capture_stderr', and 'capture_hardware_metrics'.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://app.neptune.ai/showcase/project-text-classification/e/TXTCLF-383\n"
     ]
    }
   ],
   "source": [
    "run = neptune.init_run(name=\"Keras text classification\", tags=[\"keras\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (Neptune) Log data metadata to run\n",
    "You can log nested dictionaries to create custom nested namespaces.  \n",
    "[Read the docs](https://docs.neptune.ai/logging/methods/#essential-logging-methods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_params = {\n",
    "    \"batch_size\": 32,\n",
    "    \"validation_split\": 0.2,\n",
    "    \"max_features\": 2000,\n",
    "    \"embedding_dim\": 128,\n",
    "    \"sequence_length\": 500,\n",
    "    \"seed\": 42,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run[\"data/params\"] = data_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (Neptune) Track dataset at the run-level\n",
    "We can fetch the dataset from the project metadata and track it at the run level using the `fetch()` method.  \n",
    "[`fetch()` API reference](https://docs.neptune.ai/api/field_types/#fetch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run[\"data/files\"] = project[\"keras/data/files\"].fetch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate training, validation, and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 25000 files belonging to 2 classes.\n",
      "Using 20000 files for training.\n",
      "Using 5000 files for validation.\n",
      "Found 25000 files belonging to 2 classes.\n",
      "Number of batches in raw_train_ds: 625\n",
      "Number of batches in raw_val_ds: 157\n",
      "Number of batches in raw_test_ds: 782\n"
     ]
    }
   ],
   "source": [
    "raw_train_ds, raw_val_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
    "    \"../data/train\",\n",
    "    batch_size=data_params[\"batch_size\"],\n",
    "    validation_split=data_params[\"validation_split\"],\n",
    "    subset=\"both\",\n",
    "    seed=data_params[\"seed\"],\n",
    ")\n",
    "\n",
    "raw_test_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
    "    \"../data/test\", batch_size=data_params[\"batch_size\"]\n",
    ")\n",
    "\n",
    "print(f\"Number of batches in raw_train_ds: {raw_train_ds.cardinality()}\")\n",
    "print(f\"Number of batches in raw_val_ds: {raw_val_ds.cardinality()}\")\n",
    "print(f\"Number of batches in raw_test_ds: {raw_test_ds.cardinality()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import TextVectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\siddh\\miniconda3\\envs\\py38\\lib\\site-packages\\tensorflow\\python\\autograph\\pyct\\static_analysis\\liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
     ]
    }
   ],
   "source": [
    "vectorize_layer = TextVectorization(\n",
    "    standardize=custom_standardization,\n",
    "    max_tokens=data_params[\"max_features\"],\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=data_params[\"sequence_length\"],\n",
    ")\n",
    "\n",
    "text_ds = raw_train_ds.map(lambda x, y: x)\n",
    "vectorize_layer.adapt(text_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize the data.\n",
    "train_ds = raw_train_ds.map(vectorize_text)\n",
    "val_ds = raw_val_ds.map(vectorize_text)\n",
    "test_ds = raw_test_ds.map(vectorize_text)\n",
    "\n",
    "# Do async prefetching / buffering of the data for best performance on GPU.\n",
    "train_ds = train_ds.cache().prefetch(buffer_size=10)\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=10)\n",
    "test_ds = test_ds.cache().prefetch(buffer_size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Neptune) Register a model and create a new model version\n",
    "With Neptune's model registry, you can store your ML models in a central location and collaboratively manage their lifecycle.  \n",
    "[Read the docs](https://docs.neptune.ai/how-to-guides/model-registry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://app.neptune.ai/showcase/project-text-classification/m/TXTCLF-KER/v/TXTCLF-KER-13\n"
     ]
    }
   ],
   "source": [
    "from neptune.exceptions import NeptuneModelKeyAlreadyExistsError\n",
    "\n",
    "project_key = project[\"sys/id\"].fetch()\n",
    "\n",
    "try:\n",
    "    model = neptune.init_model(name=\"keras\", key=\"KER\")\n",
    "    model.stop()\n",
    "except NeptuneModelKeyAlreadyExistsError:\n",
    "    # If it already exists, we don't have to do anything.\n",
    "    pass\n",
    "\n",
    "model_version = neptune.init_model_version(model=f\"{project_key}-KER\", name=\"keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = {\n",
    "    \"dropout\": 0.5,\n",
    "    \"strides\": 3,\n",
    "    \"activation\": \"relu\",\n",
    "    \"kernel_size\": 5,\n",
    "    \"loss\": \"binary_crossentropy\",\n",
    "    \"optimizer\": \"adam\",\n",
    "    \"metrics\": [\"accuracy\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neptune.utils import stringify_unsupported\n",
    "\n",
    "model_version[\"params\"] = stringify_unsupported(model_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_model = build_model(model_params, data_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (Neptune) Initialize the Neptune callback\n",
    "The Neptune–Keras integration logs the following metadata automatically:\n",
    "\n",
    "* Model summary\n",
    "* Parameters of the optimizer used for training the model\n",
    "* Parameters passed to Model.fit during the training\n",
    "* Current learning rate at every epoch\n",
    "* Hardware consumption and stdout/stderr output during training\n",
    "* Training code and Git information\n",
    "\n",
    "Read more about the Neptune–Keras integration here: https://docs.neptune.ai/integrations/keras/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\miniconda3\\envs\\py38\\lib\\site-packages\\neptune\\common\\warnings.py:62: NeptuneDeprecationWarning: You're importing the Neptune client library via the deprecated `neptune.new` module, which will be removed in a future release. Import directly from `neptune` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from neptune.integrations.tensorflow_keras import NeptuneCallback\n",
    "\n",
    "neptune_callback = NeptuneCallback(run=run, log_model_diagram=True, log_on_batch=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_params = {\n",
    "    \"epochs\": 2,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "625/625 [==============================] - 31s 48ms/step - loss: 0.5462 - accuracy: 0.6830 - val_loss: 0.3610 - val_accuracy: 0.8438\n",
      "Epoch 2/2\n",
      "625/625 [==============================] - 17s 27ms/step - loss: 0.3175 - accuracy: 0.8679 - val_loss: 0.3246 - val_accuracy: 0.8594\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x22b82205940>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the model using the train and test datasets.\n",
    "keras_model.fit(\n",
    "    train_ds, validation_data=val_ds, epochs=training_params[\"epochs\"], callbacks=neptune_callback\n",
    ")\n",
    "# Training parameters are logged automatically to Neptune"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 10s 13ms/step - loss: 0.3083 - accuracy: 0.8647\n"
     ]
    }
   ],
   "source": [
    "# We save the accuracy of the  model to be able to evaluate it against the current model in production later in the code\n",
    "_, curr_model_acc = keras_model.evaluate(test_ds, callbacks=neptune_callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Neptune) Associate run with model and vice-versa\n",
    "We can fetch metadata from the run's `sys` namespace and add those to the model_version to be able to link model versions with the runs that created them, and vice-versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'TXTCLF-383', 'name': 'Keras text classification', 'url': 'https://app.neptune.ai/showcase/project-text-classification/e/TXTCLF-383'}\n"
     ]
    }
   ],
   "source": [
    "run_meta = {\n",
    "    \"id\": run[\"sys/id\"].fetch(),\n",
    "    \"name\": run[\"sys/name\"].fetch(),\n",
    "    \"url\": run.get_url(),\n",
    "}\n",
    "\n",
    "print(run_meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_version[\"run\"] = run_meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'TXTCLF-KER-13', 'name': 'keras', 'url': 'https://app.neptune.ai/showcase/project-text-classification/m/TXTCLF-KER/v/TXTCLF-KER-13'}\n"
     ]
    }
   ],
   "source": [
    "model_version_meta = {\n",
    "    \"id\": model_version[\"sys/id\"].fetch(),\n",
    "    \"name\": model_version[\"sys/name\"].fetch(),\n",
    "    \"url\": model_version.get_url(),\n",
    "}\n",
    "\n",
    "print(model_version_meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run[\"training/model/meta\"] = model_version_meta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Neptune) Upload serialized model and model weights to Neptune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_version[\"serialized_model\"] = keras_model.to_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_model.save_weights(\"model_weights.h5\")\n",
    "model_version[\"model_weights\"].upload(\"model_weights.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Neptune) Update model stage\n",
    "We can update the model stage both in the app and through the API.  \n",
    "[Read the docs](https://docs.neptune.ai/model_registry/managing_stage/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_version.change_stage(\"staging\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Neptune) Wait for all operations to reach with Neptune servers\n",
    "Since Neptune sends data to servers asynchronously by default, we need to wait for operations to complete if we want to refer to fields/objects that were sent to Neptune earlier in the same code.  \n",
    "Read about the `wait()` and `sync()` methods here: https://docs.neptune.ai/logging/wait_and_sync/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_version.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Neptune) Promote best model to production"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Neptune) Fetch current champion model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://app.neptune.ai/showcase/project-text-classification/m/TXTCLF-KER\n",
      "Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "All 0 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://app.neptune.ai/showcase/project-text-classification/m/TXTCLF-KER/metadata\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sys/creation_time</th>\n",
       "      <th>sys/id</th>\n",
       "      <th>sys/model_id</th>\n",
       "      <th>sys/modification_time</th>\n",
       "      <th>sys/monitoring_time</th>\n",
       "      <th>sys/name</th>\n",
       "      <th>sys/owner</th>\n",
       "      <th>sys/ping_time</th>\n",
       "      <th>sys/running_time</th>\n",
       "      <th>sys/size</th>\n",
       "      <th>...</th>\n",
       "      <th>params/dropout</th>\n",
       "      <th>params/kernel_size</th>\n",
       "      <th>params/loss</th>\n",
       "      <th>params/metrics</th>\n",
       "      <th>params/optimizer</th>\n",
       "      <th>params/strides</th>\n",
       "      <th>run/id</th>\n",
       "      <th>run/name</th>\n",
       "      <th>run/url</th>\n",
       "      <th>serialized_model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-03-07 12:46:03.101000+00:00</td>\n",
       "      <td>TXTCLF-KER-13</td>\n",
       "      <td>TXTCLF-KER</td>\n",
       "      <td>2023-03-07 12:51:31.669000+00:00</td>\n",
       "      <td>168</td>\n",
       "      <td>keras</td>\n",
       "      <td>siddhant.sadangi</td>\n",
       "      <td>2023-03-07 12:51:31.669000+00:00</td>\n",
       "      <td>328.548</td>\n",
       "      <td>1777304.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>5</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>['accuracy']</td>\n",
       "      <td>adam</td>\n",
       "      <td>3</td>\n",
       "      <td>TXTCLF-383</td>\n",
       "      <td>Keras text classification</td>\n",
       "      <td>https://app.neptune.ai/showcase/project-text-c...</td>\n",
       "      <td>{\"class_name\": \"Functional\", \"config\": {\"name\"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-03-07 12:29:02.563000+00:00</td>\n",
       "      <td>TXTCLF-KER-12</td>\n",
       "      <td>TXTCLF-KER</td>\n",
       "      <td>2023-03-07 12:35:51.529000+00:00</td>\n",
       "      <td>171</td>\n",
       "      <td>keras</td>\n",
       "      <td>siddhant.sadangi</td>\n",
       "      <td>2023-03-07 12:36:04.880000+00:00</td>\n",
       "      <td>422.294</td>\n",
       "      <td>1777269.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>5</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>adam</td>\n",
       "      <td>2</td>\n",
       "      <td>TXTCLF-381</td>\n",
       "      <td>Keras text classification</td>\n",
       "      <td>https://app.neptune.ai/showcase/project-text-c...</td>\n",
       "      <td>{\"class_name\": \"Functional\", \"config\": {\"name\"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-03-06 16:34:05.166000+00:00</td>\n",
       "      <td>TXTCLF-KER-11</td>\n",
       "      <td>TXTCLF-KER</td>\n",
       "      <td>2023-03-06 17:31:56.266000+00:00</td>\n",
       "      <td>658</td>\n",
       "      <td>keras</td>\n",
       "      <td>siddhant.sadangi</td>\n",
       "      <td>2023-03-06 17:31:56.266000+00:00</td>\n",
       "      <td>3470.926</td>\n",
       "      <td>10993269.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>5</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>adam</td>\n",
       "      <td>2</td>\n",
       "      <td>TXTCLF-379</td>\n",
       "      <td>Keras text classification</td>\n",
       "      <td>https://app.neptune.ai/showcase/project-text-c...</td>\n",
       "      <td>{\"class_name\": \"Functional\", \"config\": {\"name\"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-01-06 12:43:16.770000+00:00</td>\n",
       "      <td>TXTCLF-KER-10</td>\n",
       "      <td>TXTCLF-KER</td>\n",
       "      <td>2023-03-07 12:35:51.465000+00:00</td>\n",
       "      <td>92</td>\n",
       "      <td>Untitled</td>\n",
       "      <td>siddhant.sadangi</td>\n",
       "      <td>2023-03-07 12:35:51.465000+00:00</td>\n",
       "      <td>3338.603</td>\n",
       "      <td>2039452.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>7</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>['accuracy']</td>\n",
       "      <td>adam</td>\n",
       "      <td>3</td>\n",
       "      <td>TXTCLF-364</td>\n",
       "      <td>Keras text classification</td>\n",
       "      <td>https://app.neptune.ai/showcase/project-text-c...</td>\n",
       "      <td>{\"class_name\": \"Functional\", \"config\": {\"name\"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-01-06 12:33:49.456000+00:00</td>\n",
       "      <td>TXTCLF-KER-9</td>\n",
       "      <td>TXTCLF-KER</td>\n",
       "      <td>2023-01-06 12:37:44.977000+00:00</td>\n",
       "      <td>231</td>\n",
       "      <td>keras</td>\n",
       "      <td>siddhant.sadangi</td>\n",
       "      <td>2023-01-06 12:37:44.977000+00:00</td>\n",
       "      <td>235.507</td>\n",
       "      <td>13422231.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>4</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>['accuracy']</td>\n",
       "      <td>adam</td>\n",
       "      <td>3</td>\n",
       "      <td>TXTCLF-362</td>\n",
       "      <td>Keras text classification</td>\n",
       "      <td>https://app.neptune.ai/showcase/project-text-c...</td>\n",
       "      <td>{\"class_name\": \"Functional\", \"config\": {\"name\"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2023-01-06 12:30:34.135000+00:00</td>\n",
       "      <td>TXTCLF-KER-8</td>\n",
       "      <td>TXTCLF-KER</td>\n",
       "      <td>2023-01-06 12:30:34.392000+00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>keras</td>\n",
       "      <td>siddhant.sadangi</td>\n",
       "      <td>2023-01-06 12:30:34.392000+00:00</td>\n",
       "      <td>0.257</td>\n",
       "      <td>251.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>4</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>['accuracy']</td>\n",
       "      <td>adam</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2023-01-06 12:18:54.909000+00:00</td>\n",
       "      <td>TXTCLF-KER-7</td>\n",
       "      <td>TXTCLF-KER</td>\n",
       "      <td>2023-01-06 12:20:52.060000+00:00</td>\n",
       "      <td>89</td>\n",
       "      <td>keras</td>\n",
       "      <td>siddhant.sadangi</td>\n",
       "      <td>2023-01-06 12:20:52.060000+00:00</td>\n",
       "      <td>117.145</td>\n",
       "      <td>11255451.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>7</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>['accuracy']</td>\n",
       "      <td>adam</td>\n",
       "      <td>3</td>\n",
       "      <td>TXTCLF-358</td>\n",
       "      <td>Keras text classification</td>\n",
       "      <td>https://app.neptune.ai/showcase/project-text-c...</td>\n",
       "      <td>{\"class_name\": \"Functional\", \"config\": {\"name\"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2023-01-05 18:25:49.330000+00:00</td>\n",
       "      <td>TXTCLF-KER-6</td>\n",
       "      <td>TXTCLF-KER</td>\n",
       "      <td>2023-01-05 18:27:25.769000+00:00</td>\n",
       "      <td>82</td>\n",
       "      <td>keras</td>\n",
       "      <td>siddhant.sadangi</td>\n",
       "      <td>2023-01-05 18:27:25.769000+00:00</td>\n",
       "      <td>96.435</td>\n",
       "      <td>11255451.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>7</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>['accuracy']</td>\n",
       "      <td>adam</td>\n",
       "      <td>3</td>\n",
       "      <td>TXTCLF-356</td>\n",
       "      <td>Keras text classification</td>\n",
       "      <td>https://app.neptune.ai/showcase/project-text-c...</td>\n",
       "      <td>{\"class_name\": \"Functional\", \"config\": {\"name\"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2023-01-05 18:09:11.474000+00:00</td>\n",
       "      <td>TXTCLF-KER-5</td>\n",
       "      <td>TXTCLF-KER</td>\n",
       "      <td>2023-01-05 18:10:57.264000+00:00</td>\n",
       "      <td>93</td>\n",
       "      <td>keras</td>\n",
       "      <td>siddhant.sadangi</td>\n",
       "      <td>2023-01-05 18:10:57.264000+00:00</td>\n",
       "      <td>105.786</td>\n",
       "      <td>11255451.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>7</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>['accuracy']</td>\n",
       "      <td>adam</td>\n",
       "      <td>3</td>\n",
       "      <td>TXTCLF-354</td>\n",
       "      <td>Keras text classification</td>\n",
       "      <td>https://app.neptune.ai/showcase/project-text-c...</td>\n",
       "      <td>{\"class_name\": \"Functional\", \"config\": {\"name\"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2023-01-05 18:00:54.375000+00:00</td>\n",
       "      <td>TXTCLF-KER-4</td>\n",
       "      <td>TXTCLF-KER</td>\n",
       "      <td>2023-01-05 18:03:31.043000+00:00</td>\n",
       "      <td>140</td>\n",
       "      <td>keras</td>\n",
       "      <td>siddhant.sadangi</td>\n",
       "      <td>2023-01-05 18:03:31.043000+00:00</td>\n",
       "      <td>156.663</td>\n",
       "      <td>11255731.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>7</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>['accuracy']</td>\n",
       "      <td>adam</td>\n",
       "      <td>3</td>\n",
       "      <td>TXTCLF-352</td>\n",
       "      <td>Keras text classification</td>\n",
       "      <td>https://app.neptune.ai/showcase/project-text-c...</td>\n",
       "      <td>{\"class_name\": \"Functional\", \"config\": {\"name\"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2023-01-05 17:57:35.457000+00:00</td>\n",
       "      <td>TXTCLF-KER-3</td>\n",
       "      <td>TXTCLF-KER</td>\n",
       "      <td>2023-01-06 12:45:01.920000+00:00</td>\n",
       "      <td>137</td>\n",
       "      <td>Untitled</td>\n",
       "      <td>siddhant.sadangi</td>\n",
       "      <td>2023-01-06 12:45:01.920000+00:00</td>\n",
       "      <td>191.548</td>\n",
       "      <td>11255454.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>7</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>['accuracy']</td>\n",
       "      <td>adam</td>\n",
       "      <td>3</td>\n",
       "      <td>TXTCLF-350</td>\n",
       "      <td>Keras text classification</td>\n",
       "      <td>https://app.neptune.ai/showcase/project-text-c...</td>\n",
       "      <td>{\"class_name\": \"Functional\", \"config\": {\"name\"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2023-01-05 17:35:27.859000+00:00</td>\n",
       "      <td>TXTCLF-KER-2</td>\n",
       "      <td>TXTCLF-KER</td>\n",
       "      <td>2023-01-05 18:00:09.752000+00:00</td>\n",
       "      <td>123</td>\n",
       "      <td>Untitled</td>\n",
       "      <td>siddhant.sadangi</td>\n",
       "      <td>2023-01-05 18:16:47.766000+00:00</td>\n",
       "      <td>2479.781</td>\n",
       "      <td>11255454.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>7</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>['accuracy']</td>\n",
       "      <td>adam</td>\n",
       "      <td>3</td>\n",
       "      <td>TXTCLF-348</td>\n",
       "      <td>Keras text classification</td>\n",
       "      <td>https://app.neptune.ai/showcase/project-text-c...</td>\n",
       "      <td>{\"class_name\": \"Functional\", \"config\": {\"name\"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2023-01-05 17:26:18.799000+00:00</td>\n",
       "      <td>TXTCLF-KER-1</td>\n",
       "      <td>TXTCLF-KER</td>\n",
       "      <td>2023-01-05 17:39:34.330000+00:00</td>\n",
       "      <td>49</td>\n",
       "      <td>Untitled</td>\n",
       "      <td>siddhant.sadangi</td>\n",
       "      <td>2023-01-05 18:16:47.943000+00:00</td>\n",
       "      <td>2383.721</td>\n",
       "      <td>4529.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>7</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>['accuracy']</td>\n",
       "      <td>adam</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{\"class_name\": \"Functional\", \"config\": {\"name\"...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  sys/creation_time         sys/id sys/model_id  \\\n",
       "0  2023-03-07 12:46:03.101000+00:00  TXTCLF-KER-13   TXTCLF-KER   \n",
       "1  2023-03-07 12:29:02.563000+00:00  TXTCLF-KER-12   TXTCLF-KER   \n",
       "2  2023-03-06 16:34:05.166000+00:00  TXTCLF-KER-11   TXTCLF-KER   \n",
       "3  2023-01-06 12:43:16.770000+00:00  TXTCLF-KER-10   TXTCLF-KER   \n",
       "4  2023-01-06 12:33:49.456000+00:00   TXTCLF-KER-9   TXTCLF-KER   \n",
       "5  2023-01-06 12:30:34.135000+00:00   TXTCLF-KER-8   TXTCLF-KER   \n",
       "6  2023-01-06 12:18:54.909000+00:00   TXTCLF-KER-7   TXTCLF-KER   \n",
       "7  2023-01-05 18:25:49.330000+00:00   TXTCLF-KER-6   TXTCLF-KER   \n",
       "8  2023-01-05 18:09:11.474000+00:00   TXTCLF-KER-5   TXTCLF-KER   \n",
       "9  2023-01-05 18:00:54.375000+00:00   TXTCLF-KER-4   TXTCLF-KER   \n",
       "10 2023-01-05 17:57:35.457000+00:00   TXTCLF-KER-3   TXTCLF-KER   \n",
       "11 2023-01-05 17:35:27.859000+00:00   TXTCLF-KER-2   TXTCLF-KER   \n",
       "12 2023-01-05 17:26:18.799000+00:00   TXTCLF-KER-1   TXTCLF-KER   \n",
       "\n",
       "              sys/modification_time  sys/monitoring_time  sys/name  \\\n",
       "0  2023-03-07 12:51:31.669000+00:00                  168     keras   \n",
       "1  2023-03-07 12:35:51.529000+00:00                  171     keras   \n",
       "2  2023-03-06 17:31:56.266000+00:00                  658     keras   \n",
       "3  2023-03-07 12:35:51.465000+00:00                   92  Untitled   \n",
       "4  2023-01-06 12:37:44.977000+00:00                  231     keras   \n",
       "5  2023-01-06 12:30:34.392000+00:00                    1     keras   \n",
       "6  2023-01-06 12:20:52.060000+00:00                   89     keras   \n",
       "7  2023-01-05 18:27:25.769000+00:00                   82     keras   \n",
       "8  2023-01-05 18:10:57.264000+00:00                   93     keras   \n",
       "9  2023-01-05 18:03:31.043000+00:00                  140     keras   \n",
       "10 2023-01-06 12:45:01.920000+00:00                  137  Untitled   \n",
       "11 2023-01-05 18:00:09.752000+00:00                  123  Untitled   \n",
       "12 2023-01-05 17:39:34.330000+00:00                   49  Untitled   \n",
       "\n",
       "           sys/owner                    sys/ping_time  sys/running_time  \\\n",
       "0   siddhant.sadangi 2023-03-07 12:51:31.669000+00:00           328.548   \n",
       "1   siddhant.sadangi 2023-03-07 12:36:04.880000+00:00           422.294   \n",
       "2   siddhant.sadangi 2023-03-06 17:31:56.266000+00:00          3470.926   \n",
       "3   siddhant.sadangi 2023-03-07 12:35:51.465000+00:00          3338.603   \n",
       "4   siddhant.sadangi 2023-01-06 12:37:44.977000+00:00           235.507   \n",
       "5   siddhant.sadangi 2023-01-06 12:30:34.392000+00:00             0.257   \n",
       "6   siddhant.sadangi 2023-01-06 12:20:52.060000+00:00           117.145   \n",
       "7   siddhant.sadangi 2023-01-05 18:27:25.769000+00:00            96.435   \n",
       "8   siddhant.sadangi 2023-01-05 18:10:57.264000+00:00           105.786   \n",
       "9   siddhant.sadangi 2023-01-05 18:03:31.043000+00:00           156.663   \n",
       "10  siddhant.sadangi 2023-01-06 12:45:01.920000+00:00           191.548   \n",
       "11  siddhant.sadangi 2023-01-05 18:16:47.766000+00:00          2479.781   \n",
       "12  siddhant.sadangi 2023-01-05 18:16:47.943000+00:00          2383.721   \n",
       "\n",
       "      sys/size  ... params/dropout params/kernel_size          params/loss  \\\n",
       "0    1777304.0  ...            0.5                  5  binary_crossentropy   \n",
       "1    1777269.0  ...            0.5                  5  binary_crossentropy   \n",
       "2   10993269.0  ...            0.5                  5  binary_crossentropy   \n",
       "3    2039452.0  ...            0.5                  7  binary_crossentropy   \n",
       "4   13422231.0  ...            0.5                  4  binary_crossentropy   \n",
       "5        251.0  ...            0.5                  4  binary_crossentropy   \n",
       "6   11255451.0  ...            0.5                  7  binary_crossentropy   \n",
       "7   11255451.0  ...            0.5                  7  binary_crossentropy   \n",
       "8   11255451.0  ...            0.5                  7  binary_crossentropy   \n",
       "9   11255731.0  ...            0.5                  7  binary_crossentropy   \n",
       "10  11255454.0  ...            0.5                  7  binary_crossentropy   \n",
       "11  11255454.0  ...            0.5                  7  binary_crossentropy   \n",
       "12      4529.0  ...            0.5                  7  binary_crossentropy   \n",
       "\n",
       "    params/metrics params/optimizer  params/strides      run/id  \\\n",
       "0     ['accuracy']             adam               3  TXTCLF-383   \n",
       "1              NaN             adam               2  TXTCLF-381   \n",
       "2              NaN             adam               2  TXTCLF-379   \n",
       "3     ['accuracy']             adam               3  TXTCLF-364   \n",
       "4     ['accuracy']             adam               3  TXTCLF-362   \n",
       "5     ['accuracy']             adam               3         NaN   \n",
       "6     ['accuracy']             adam               3  TXTCLF-358   \n",
       "7     ['accuracy']             adam               3  TXTCLF-356   \n",
       "8     ['accuracy']             adam               3  TXTCLF-354   \n",
       "9     ['accuracy']             adam               3  TXTCLF-352   \n",
       "10    ['accuracy']             adam               3  TXTCLF-350   \n",
       "11    ['accuracy']             adam               3  TXTCLF-348   \n",
       "12    ['accuracy']             adam               3         NaN   \n",
       "\n",
       "                     run/name  \\\n",
       "0   Keras text classification   \n",
       "1   Keras text classification   \n",
       "2   Keras text classification   \n",
       "3   Keras text classification   \n",
       "4   Keras text classification   \n",
       "5                         NaN   \n",
       "6   Keras text classification   \n",
       "7   Keras text classification   \n",
       "8   Keras text classification   \n",
       "9   Keras text classification   \n",
       "10  Keras text classification   \n",
       "11  Keras text classification   \n",
       "12                        NaN   \n",
       "\n",
       "                                              run/url  \\\n",
       "0   https://app.neptune.ai/showcase/project-text-c...   \n",
       "1   https://app.neptune.ai/showcase/project-text-c...   \n",
       "2   https://app.neptune.ai/showcase/project-text-c...   \n",
       "3   https://app.neptune.ai/showcase/project-text-c...   \n",
       "4   https://app.neptune.ai/showcase/project-text-c...   \n",
       "5                                                 NaN   \n",
       "6   https://app.neptune.ai/showcase/project-text-c...   \n",
       "7   https://app.neptune.ai/showcase/project-text-c...   \n",
       "8   https://app.neptune.ai/showcase/project-text-c...   \n",
       "9   https://app.neptune.ai/showcase/project-text-c...   \n",
       "10  https://app.neptune.ai/showcase/project-text-c...   \n",
       "11  https://app.neptune.ai/showcase/project-text-c...   \n",
       "12                                                NaN   \n",
       "\n",
       "                                     serialized_model  \n",
       "0   {\"class_name\": \"Functional\", \"config\": {\"name\"...  \n",
       "1   {\"class_name\": \"Functional\", \"config\": {\"name\"...  \n",
       "2   {\"class_name\": \"Functional\", \"config\": {\"name\"...  \n",
       "3   {\"class_name\": \"Functional\", \"config\": {\"name\"...  \n",
       "4   {\"class_name\": \"Functional\", \"config\": {\"name\"...  \n",
       "5                                                 NaN  \n",
       "6   {\"class_name\": \"Functional\", \"config\": {\"name\"...  \n",
       "7   {\"class_name\": \"Functional\", \"config\": {\"name\"...  \n",
       "8   {\"class_name\": \"Functional\", \"config\": {\"name\"...  \n",
       "9   {\"class_name\": \"Functional\", \"config\": {\"name\"...  \n",
       "10  {\"class_name\": \"Functional\", \"config\": {\"name\"...  \n",
       "11  {\"class_name\": \"Functional\", \"config\": {\"name\"...  \n",
       "12  {\"class_name\": \"Functional\", \"config\": {\"name\"...  \n",
       "\n",
       "[13 rows x 25 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with neptune.init_model(with_id=f\"{project_key}-KER\") as model:\n",
    "    model_versions_df = model.fetch_model_versions_table().to_pandas()\n",
    "model_versions_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "production_models = model_versions_df[model_versions_df[\"sys/stage\"] == \"production\"][\"sys/id\"]\n",
    "assert (\n",
    "    len(production_models) == 1\n",
    "), f\"Multiple model versions found in production: {production_models.values}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current model in production: TXTCLF-KER-12\n"
     ]
    }
   ],
   "source": [
    "prod_model_id = production_models.values[0]\n",
    "print(f\"Current model in production: {prod_model_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://app.neptune.ai/showcase/project-text-classification/m/TXTCLF-KER/v/TXTCLF-KER-12\n"
     ]
    }
   ],
   "source": [
    "npt_prod_model = neptune.init_model_version(with_id=prod_model_id)\n",
    "npt_prod_model_params = npt_prod_model[\"params\"].fetch()\n",
    "prod_model = tf.keras.models.model_from_json(npt_prod_model[\"serialized_model\"].fetch())\n",
    "\n",
    "npt_prod_model[\"model_weights\"].download()\n",
    "prod_model.load_weights(\"model_weights.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Neptune) Evaluate current model on lastest test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (Neptune) Fetch data parameters from the run that created the model to preserve data preprocessing\n",
    "We reinitialize the run that created the current champion model to fetch the data parameters by passing it's ID to the `with_id` parameter of `init_run()`.  \n",
    "`with_id`'s API reference: https://docs.neptune.ai/api/universal/#with_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://app.neptune.ai/showcase/project-text-classification/e/TXTCLF-381\n",
      "{'batch_size': 32, 'embedding_dim': 128, 'max_features': 2000, 'seed': 42, 'sequence_length': 500, 'validation_split': 0.2}\n"
     ]
    }
   ],
   "source": [
    "prod_run_id = npt_prod_model[\"run/id\"].fetch()\n",
    "\n",
    "prod_run = neptune.init_run(with_id=prod_run_id)\n",
    "prod_data_params = prod_run[\"data/params\"].fetch()\n",
    "\n",
    "print(prod_data_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparing test data according to fetched data parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 25000 files belonging to 2 classes.\n",
      "Number of batches in raw_test_ds: 782\n"
     ]
    }
   ],
   "source": [
    "raw_test_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
    "    \"../data/test\", batch_size=prod_data_params[\"batch_size\"]\n",
    ")\n",
    "\n",
    "\n",
    "print(f\"Number of batches in raw_test_ds: {raw_test_ds.cardinality()}\")\n",
    "\n",
    "vectorize_layer = TextVectorization(\n",
    "    standardize=custom_standardization,\n",
    "    max_tokens=prod_data_params[\"max_features\"],\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=prod_data_params[\"sequence_length\"],\n",
    ")\n",
    "\n",
    "text_ds = raw_train_ds.map(lambda x, y: x)\n",
    "vectorize_layer.adapt(text_ds)\n",
    "\n",
    "# Vectorize the data.\n",
    "test_ds = raw_test_ds.map(vectorize_text)\n",
    "\n",
    "# Do async prefetching / buffering of the data for best performance on GPU.\n",
    "test_ds = test_ds.cache().prefetch(buffer_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 11s 14ms/step - loss: 0.3037 - accuracy: 0.8700\n"
     ]
    }
   ],
   "source": [
    "# Evaluate champion model using the model's original loss and optimizer, but the current metric\n",
    "prod_model.compile(\n",
    "    loss=npt_prod_model_params[\"loss\"],\n",
    "    optimizer=npt_prod_model_params[\"optimizer\"],\n",
    "    metrics=model_params[\"metrics\"],\n",
    ")\n",
    "\n",
    "_, prod_model_acc = prod_model.evaluate(test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Neptune) If challenger model outperforms production model, promote it to production and mark it's run as the new `prod` run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Champion model accuracy: 0.8700000047683716\n",
      "Challenger model accuracy: 0.8647199869155884\n",
      "Archiving challenger model\n",
      "Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "All 0 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://app.neptune.ai/showcase/project-text-classification/m/TXTCLF-KER/v/TXTCLF-KER-12/metadata\n"
     ]
    }
   ],
   "source": [
    "print(f\"Champion model accuracy: {prod_model_acc}\\nChallenger model accuracy: {curr_model_acc}\")\n",
    "\n",
    "if curr_model_acc > prod_model_acc:\n",
    "    print(\"Promoting challenger to champion\")\n",
    "    npt_prod_model.change_stage(\"archived\")\n",
    "    model_version.change_stage(\"production\")\n",
    "    prod_run[\"sys/tags\"].remove(\"prod\")\n",
    "    run[\"sys/tags\"].add(\"prod\")\n",
    "else:\n",
    "    print(\"Archiving challenger model\")\n",
    "    model_version.change_stage(\"archived\")\n",
    "\n",
    "npt_prod_model.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Neptune) Stop tracking\n",
    "When working in an interactive notebook environment, we need to explicitly stop all initialized neptune objects to prevent unnecessary monitoring.  \n",
    "Read more about stopping neptune objects here: https://docs.neptune.ai/usage/best_practices/#stopping-runs-and-other-objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "All 0 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://app.neptune.ai/showcase/project-text-classification/m/TXTCLF-KER/v/TXTCLF-KER-13/metadata\n",
      "Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "All 0 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://app.neptune.ai/showcase/project-text-classification/e/TXTCLF-381/metadata\n",
      "Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "All 0 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://app.neptune.ai/showcase/project-text-classification/e/TXTCLF-383/metadata\n",
      "Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "All 0 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://app.neptune.ai/showcase/project-text-classification/metadata\n"
     ]
    }
   ],
   "source": [
    "model_version.stop()\n",
    "prod_run.stop()\n",
    "run.stop()\n",
    "project.stop()"
   ]
  }
 ],
 "metadata": {
  "keep_output": true,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "neptune": {
   "notebookId": "98a177ec-a21b-404a-ad14-def08e56f560",
   "projectVersion": 2
  },
  "vscode": {
   "interpreter": {
    "hash": "a9715cf0b0024f6e1c62cb31a4f1f43970eb41991212681878768b4bfe53050a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
