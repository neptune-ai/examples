{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "header",
     "comment"
    ]
   },
   "source": [
    "# Using the ðŸ¤— Transformers Integration\n",
    "\n",
    "Neptune provides an integration with ðŸ¤— Transformers. All you need to do to log metadata of the ðŸ¤— Transformers training/finetuning is add a few lines of additional code.\n",
    "\n",
    "You can integrate metadata tracking with Neptune either by:\n",
    "* passing `report_to='neptune'` to the Trainer arguments,\n",
    "* setting up a Neptune callback and passing it to the Trainer callbacks.\n",
    "\n",
    "In this guide, we will look at both options to logging metadata.\n",
    "\n",
    "By the end of this guide, you will be able to use the ðŸ¤— Transformers integration to log:\n",
    "* Train loss\n",
    "* Evaluation loss\n",
    "* Trainer parameters\n",
    "* Model parameters\n",
    "* Model checkpoint\n",
    "\n",
    "[See this example in Neptune](https://app.neptune.ai/o/showcase/org/project-text-summarization-hf/e/PROJ-138/dashboard/Custom-Dashboard-97370bc5-ee32-48ce-a630-fc806a370e13)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before you start\n",
    "\n",
    "This notebook example lets you try out Neptune anonymously, with zero setup.\n",
    "\n",
    "* If you're running the notebook on your local machine, you need to have [Python](https://www.python.org/downloads/) and [pip](https://pypi.org/project/pip/) installed.\n",
    "* If you want to see the example logged to your own workspace instead:\n",
    "    * Create a Neptune account â†’ [Take me to registration](https://neptune.ai/register)\n",
    "    * Create a Neptune project that you will use for tracking metadata â†’ [Tell me more about projects](https://docs.neptune.ai/administration/projects)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Neptune and dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install neptune-client transformers==4.22.0 datasets==2.5.1 torch==1.11 scipy==1.7.2 scikit-learn==1.0.1 numpy==1.22.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "tags": [
     "code"
    ]
   },
   "outputs": [],
   "source": [
    "import neptune.new as neptune\n",
    "\n",
    "project = \"common/huggingface-integration\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up model and data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, load_metric\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from transformers.integrations import NeptuneCallback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = \"cola\"\n",
    "model_checkpoint = \"prajjwal1/bert-tiny\"\n",
    "batch_size = 16\n",
    "dataset = load_dataset(\"glue\", task)\n",
    "metric = load_metric('glue', task)\n",
    "num_labels = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Tokenizer for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples['sentence'], truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_dataset = dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate the model for finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=num_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (Option 1) Using `report_to='neptune'` to TrainingArguments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we pass `report_to='neptune'`, the integration takes care of creating a Neptune `run` to log the metadata. To use `report_to` approach, we need to set the `NEPTUNE_API_TOKEN` and `NEPTUNE_PROJECT` environment variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"NEPTUNE_API_TOKEN\"] = neptune.ANONYMOUS_API_TOKEN\n",
    "os.environ[\"NEPTUNE_PROJECT\"] = project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "\n",
    "args = TrainingArguments(\n",
    "    f\"{model_name}-finetuned-{task}\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    save_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    report_to=\"neptune\",\n",
    ")\n",
    "\n",
    "validation_key = 'validation'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=encoded_dataset[\"train\"],\n",
    "    eval_dataset=encoded_dataset[validation_key],\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (Option 2) Setting up the NeptuneCallback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the training arguments for model finetuning.  \n",
    "In this case, we set `report_to='none'` so that Transformers does not create a Callback for us like above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "\n",
    "args = TrainingArguments(\n",
    "    f\"{model_name}-finetuned-{task}\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    save_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "validation_key = 'validation'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Start a run\n",
    "\n",
    "To connect your script to Neptune and create a new run, we tell Neptune:\n",
    "* **Who you are** - with a Neptune API token\n",
    "* **Where to send your data** - to a Neptune project\n",
    "\n",
    "The cell below lets you record data to the public project [common/quickstarts](https://app.neptune.ai/common/quickstarts) as an anonymous user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import neptune.new as neptune\n",
    "\n",
    "project = \"common/huggingface-integration\"\n",
    "run = neptune.init_run(\n",
    "    api_token=neptune.ANONYMOUS_API_TOKEN,\n",
    "    project=project,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, you can log the example to your own workspace.\n",
    "\n",
    "To do that, replace the code above with the following:\n",
    "\n",
    "```python\n",
    "from getpass import getpass\n",
    "my_api_token = getpass('Enter your Neptune API token: ')\n",
    "\n",
    "run = neptune.init_run(\n",
    "    api_token=my_api_token,\n",
    "    project=\"workspace-name/project-name\",  # replace with your own\n",
    ")\n",
    "```\n",
    "\n",
    "For example, if your workspace name is `ml-team` and the project name is `classification`, the project argument is: `project=\"ml-team/classification\"`\n",
    "\n",
    "To find your API token and project name, [log in to Neptune](https://app.neptune.ai/).\n",
    "- In the top-right corner, click your avatar and select **Get your API token**.\n",
    "- To find and copy your project name, navigate to the project, then click **Settings** â†’ **Properties**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Instantiate NeptuneCallback\n",
    "\n",
    "We pass the `run` to the Neptune callback. The callback will take care of logging the metadata during the training phase. You can customize the metadata that is logged by passing additional arguments to the callback.\n",
    "\n",
    "See https://docs.neptune.ai/integrations-and-supported-tools/model-training/hugging-face for more details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate the NeptuneCallback with our `run`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neptune_callback = NeptuneCallback(run=run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=encoded_dataset[\"train\"],\n",
    "    eval_dataset=encoded_dataset[validation_key],\n",
    "    callbacks=[neptune_callback],\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: The `run` object is stopped once `Trainer.train()` is finished. As such, we don't need to call `run.stop()` explicitly (which is otherwise required in interactive environments, such as Jupyter Notebook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "comment"
    ]
   },
   "source": [
    "## Explore the results in Neptune\n",
    "\n",
    "We just finetuned our model with the new data. Let's see an example of the data that was logged to Neptune.\n",
    "\n",
    "You can also check out an [example run](https://app.neptune.ai/o/showcase/org/project-text-summarization-hf/e/PROJ-138/dashboard/Custom-Dashboard-97370bc5-ee32-48ce-a630-fc806a370e13)."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "FUaDrmRu-h3k"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.10.5 ('neptune')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "ec001e92a202bf5caf822e845ea5ad1be0f2e89477ccf084fef05c1d70e5b02a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
